{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize  \n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)\n",
    "#### changed below line\n",
    "#tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Added a dot infront of the path\n",
    "train_doc = np.random.permutation(open('./Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('./Project_data/val.csv').readlines())\n",
    "####train_doc = np.random.permutation(open('/notebooks/storage/Final_data/Collated_training/train.csv').readlines())\n",
    "####val_doc = np.random.permutation(open('/notebooks/storage/Final_data/Collated_training/val.csv').readlines())\n",
    "\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Initial phase test workload so that we can test the basic models and try overfitting\n",
    "##train_doc = train_doc[:120]\n",
    "##val_doc = val_doc[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### adding an extra parameter of the image_idx so that i don't have to compile this generator function again and again\n",
    "\n",
    "def generator(source_path, folder_list, batch_size,img_cnt_idx=30):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    #### initially chosing 30 images , will latre reduce it to 10 or 20 images\n",
    "    img_idx = [x for x in range( (30 - img_cnt_idx ),30)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size   # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    ### Not trying cropping because the images in the videos are from extreme left to extreme right so don;t want to lose info\n",
    "                    image_resized=imresize(image,(100,100,3))\n",
    "                    \n",
    "                    \n",
    "                    ### Tried different normalization visually both looked same, but my local machine GPU crashed while running on first one so avoiding it for now\n",
    "                    ##image_resized - np.percentile(image_resized,5)/ np.percentile(image_resized,95) - np.percentile(image_resized,5)\n",
    "                    ##(image_resized[:,:,0])/255\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (image_resized[:,:,0])/255  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (image_resized[:,:,1])/255  #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image_resized[:,:,2])/255  #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        rem_num_batches = len(folder_list)%batch_size \n",
    "        if (rem_num_batches != 0) :\n",
    "            for batch in range(rem_num_batches): # we iterate over the number of batches\n",
    "                batch_data = np.zeros((batch_size,len(img_idx),100,100,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "                batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "                for folder in range(batch_size): # iterate over the batch_size\n",
    "                    imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                    for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                        image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                        \n",
    "                        #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                        #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                        image_resized=imresize(image,(100,100,3))\n",
    "                        \n",
    "                        batch_data[folder,idx,:,:,0] =  (image_resized[:,:,0])/255  #normalise and feed in the image\n",
    "                        batch_data[folder,idx,:,:,1] =  (image_resized[:,:,1])/255  #normalise and feed in the image\n",
    "                        batch_data[folder,idx,:,:,2] =  (image_resized[:,:,2])/255  #normalise and feed in the image\n",
    "                    batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "#### Adding a dot infront of the path\n",
    "train_path = './Project_data/train'\n",
    "val_path = './Project_data/val'\n",
    "####train_path = '/notebooks/storage/Final_data/Collated_training/train'\n",
    "####val_path = '/notebooks/storage/Final_data/Collated_training/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10                             # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D,Conv2D,MaxPooling2D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "#write your model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8,kernel_size=(3,3,3), padding='same',input_shape=(24,100,100,3)))   #### 30 in case of full , image size 120,120\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(16, kernel_size=(3,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size=(1,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(1,3,3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 24, 100, 100, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24, 100, 100, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 24, 100, 100, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 12, 50, 50, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 12, 50, 50, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 12, 50, 50, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 6, 25, 25, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 6, 25, 25, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 6, 25, 25, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 6, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 3, 12, 12, 64)     18496     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 6, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 651,109\n",
      "Trainable params: 650,997\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam()                                   #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20 \n",
    "img_cnt_idx=24\n",
    "train_generator = generator(train_path, train_doc, batch_size,img_cnt_idx)\n",
    "val_generator = generator(val_path, val_doc, batch_size,img_cnt_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)   # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 20\n",
      "Source path =  ./Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 9s - loss: 1.3175 - categorical_accuracy: 0.4281 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 155s 5s/step - loss: 1.3111 - categorical_accuracy: 0.4324 - val_loss: 1.0568 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-09-1212_14_36.139518/model-00001-1.31107-0.43235-1.05684-0.51000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 1.1882 - categorical_accuracy: 0.4544 - val_loss: 1.0044 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-09-1212_14_36.139518/model-00002-1.18819-0.45441-1.00436-0.58000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 1.0485 - categorical_accuracy: 0.5559 - val_loss: 1.0268 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-09-1212_14_36.139518/model-00003-1.04846-0.55588-1.02682-0.58000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.8948 - categorical_accuracy: 0.6279 - val_loss: 0.8182 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-09-1212_14_36.139518/model-00004-0.89475-0.62794-0.81823-0.72000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.8155 - categorical_accuracy: 0.6912 - val_loss: 1.2939 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-09-1212_14_36.139518/model-00005-0.81550-0.69118-1.29387-0.57000.h5\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.7517 - categorical_accuracy: 0.7000 - val_loss: 0.7717 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-09-1212_14_36.139518/model-00006-0.75175-0.70000-0.77167-0.68000.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.6786 - categorical_accuracy: 0.7368 - val_loss: 0.9027 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-09-1212_14_36.139518/model-00007-0.67864-0.73676-0.90268-0.61000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.5969 - categorical_accuracy: 0.7676 - val_loss: 0.9195 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-09-1212_14_36.139518/model-00008-0.59691-0.76765-0.91953-0.61000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.5736 - categorical_accuracy: 0.7676 - val_loss: 0.7605 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-09-1212_14_36.139518/model-00009-0.57363-0.76765-0.76054-0.66000.h5\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.4712 - categorical_accuracy: 0.8338 - val_loss: 0.6769 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-09-1212_14_36.139518/model-00010-0.47115-0.83382-0.67692-0.75000.h5\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.4208 - categorical_accuracy: 0.8235 - val_loss: 0.6823 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-09-1212_14_36.139518/model-00011-0.42084-0.82353-0.68227-0.83000.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 56s 2s/step - loss: 0.3661 - categorical_accuracy: 0.8632 - val_loss: 0.7266 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-09-1212_14_36.139518/model-00012-0.36610-0.86324-0.72655-0.74000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.2909 - categorical_accuracy: 0.8971 - val_loss: 0.7556 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-09-1212_14_36.139518/model-00013-0.29086-0.89706-0.75561-0.75000.h5\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.2609 - categorical_accuracy: 0.9147 - val_loss: 0.7335 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-09-1212_14_36.139518/model-00014-0.26091-0.91471-0.73350-0.71000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.1526 - categorical_accuracy: 0.9559 - val_loss: 0.6500 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-09-1212_14_36.139518/model-00015-0.15259-0.95588-0.64997-0.78000.h5\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.1253 - categorical_accuracy: 0.9647 - val_loss: 0.6308 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-09-1212_14_36.139518/model-00016-0.12529-0.96471-0.63085-0.78000.h5\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.1467 - categorical_accuracy: 0.9574 - val_loss: 0.6451 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-09-1212_14_36.139518/model-00017-0.14667-0.95735-0.64512-0.78000.h5\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.1030 - categorical_accuracy: 0.9750 - val_loss: 0.6264 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-09-1212_14_36.139518/model-00018-0.10297-0.97500-0.62638-0.74000.h5\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 55s 2s/step - loss: 0.0965 - categorical_accuracy: 0.9750 - val_loss: 0.6130 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-09-1212_14_36.139518/model-00019-0.09652-0.97500-0.61298-0.81000.h5\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 54s 2s/step - loss: 0.0813 - categorical_accuracy: 0.9765 - val_loss: 0.6037 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-09-1212_14_36.139518/model-00020-0.08127-0.97647-0.60372-0.78000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c37ae3a58>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>So for a model with 6 layers and input of 16 images  and image files of (120,120) we have an accuracy of 84% in training and 38% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>So for a model with 6 layers and input of 24 images and image files of (120,120)  we have an accuracy of 82% in training and 57% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>So for a model with 6 layers and input of 24 images and image files of (100,100) we have an accuracy of 85% in training and 43% in validation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>So for a model with 6 layers and input of 24 images and image files of (100,100) and num_epochs = 20 we have an accuracy of 97.65% in training and 78% in validation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style ='background:yellow'>Model 2: Reducing 1 layer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_9 (Conv3D)            (None, 24, 100, 100, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 24, 100, 100, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 24, 100, 100, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3 (None, 12, 50, 50, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 12, 50, 50, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 12, 50, 50, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 12, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling (None, 6, 25, 25, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 6, 25, 25, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 6, 25, 25, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 6, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 13824)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               3539200   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 3,581,733\n",
      "Trainable params: 3,581,621\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#write your model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8,kernel_size=(3,3,3), padding='same',input_shape=(24,100,100,3)))   #### 30 in case of full , image size 120,120\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(16, kernel_size=(3,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size=(1,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = optimizers.Adam()                                   #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20 \n",
    "img_cnt_idx=24\n",
    "train_generator = generator(train_path, train_doc, batch_size,img_cnt_idx)\n",
    "val_generator = generator(val_path, val_doc, batch_size,img_cnt_idx)\n",
    "\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)   # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 20\n",
      "Source path =  ./Project_data/trainEpoch 1/20\n",
      " ; batch size = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 3s - loss: 11.2524 - categorical_accuracy: 0.2172"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 65s 2s/step - loss: 11.3334 - categorical_accuracy: 0.2147 - val_loss: 12.7908 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-09-1212_14_36.139518/model-00001-11.33340-0.21471-12.79075-0.18000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 11.5401 - categorical_accuracy: 0.2412 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-09-1212_14_36.139518/model-00002-11.54010-0.24118-12.41093-0.23000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 10.3415 - categorical_accuracy: 0.3088 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-09-1212_14_36.139518/model-00003-10.34147-0.30882-12.41093-0.23000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 10.6133 - categorical_accuracy: 0.2971 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-09-1212_14_36.139518/model-00004-10.61325-0.29706-12.41093-0.23000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 10.1421 - categorical_accuracy: 0.3265 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-09-1212_14_36.139518/model-00005-10.14212-0.32647-12.41093-0.23000.h5\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 10.0561 - categorical_accuracy: 0.3265 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-09-1212_14_36.139518/model-00006-10.05605-0.32647-12.41093-0.23000.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 9.3445 - categorical_accuracy: 0.3647 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-09-1212_14_36.139518/model-00007-9.34452-0.36471-12.41093-0.23000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 9.6190 - categorical_accuracy: 0.3441 - val_loss: 12.1515 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-09-1212_14_36.139518/model-00008-9.61900-0.34412-12.15149-0.24000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 9.2562 - categorical_accuracy: 0.3603 - val_loss: 8.7829 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-09-1212_14_36.139518/model-00009-9.25622-0.36029-8.78292-0.37000.h5\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 9.4672 - categorical_accuracy: 0.3500 - val_loss: 7.1349 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-09-1212_14_36.139518/model-00010-9.46717-0.35000-7.13492-0.43000.h5\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 8.5657 - categorical_accuracy: 0.3838 - val_loss: 6.5962 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-09-1212_14_36.139518/model-00011-8.56570-0.38382-6.59617-0.42000.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 8.9067 - categorical_accuracy: 0.3515 - val_loss: 8.3445 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-09-1212_14_36.139518/model-00012-8.90666-0.35147-8.34450-0.34000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 57s 2s/step - loss: 8.1787 - categorical_accuracy: 0.4044 - val_loss: 9.1776 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-09-1212_14_36.139518/model-00013-8.17874-0.40441-9.17758-0.30000.h5\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 57s 2s/step - loss: 8.4542 - categorical_accuracy: 0.3882 - val_loss: 8.0816 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-09-1212_14_36.139518/model-00014-8.45419-0.38824-8.08163-0.32000.h5\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 55s 2s/step - loss: 7.9368 - categorical_accuracy: 0.3926 - val_loss: 7.5292 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-09-1212_14_36.139518/model-00015-7.93679-0.39265-7.52916-0.34000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 7.2677 - categorical_accuracy: 0.4265 - val_loss: 5.6343 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-09-1212_14_36.139518/model-00016-7.26768-0.42647-5.63425-0.40000.h5\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 7.2722 - categorical_accuracy: 0.3956 - val_loss: 4.8349 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-09-1212_14_36.139518/model-00017-7.27220-0.39559-4.83490-0.48000.h5\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 7.6659 - categorical_accuracy: 0.3721 - val_loss: 4.5485 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-09-1212_14_36.139518/model-00018-7.66592-0.37206-4.54848-0.49000.h5\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 7.1219 - categorical_accuracy: 0.3809 - val_loss: 4.2624 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-09-1212_14_36.139518/model-00019-7.12189-0.38088-4.26244-0.52000.h5\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 6.8076 - categorical_accuracy: 0.4059 - val_loss: 4.0432 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-09-1212_14_36.139518/model-00020-6.80757-0.40588-4.04316-0.54000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c205fb588>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>By reducing the number of layers the accuracy also decreased : train to 28% and val to 36%</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>Even increasing the number of images didn't have much effect and the accuracy was down at train : 24% and val : 30%</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>with batch size 20 and epoch = 20 and 24 images at (100,100)  the accuracy was for train : 40% and val : 54% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='background:yellow'>Model 3:   Trying another model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_17 (Conv3D)           (None, 24, 100, 100, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 24, 100, 100, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 24, 100, 100, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_16 (MaxPooling (None, 12, 50, 50, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_18 (Conv3D)           (None, 12, 50, 50, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 12, 50, 50, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 12, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_19 (Conv3D)           (None, 12, 50, 50, 16)    6928      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 12, 50, 50, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 12, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_17 (MaxPooling (None, 6, 25, 25, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_20 (Conv3D)           (None, 6, 25, 25, 32)     13856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 6, 25, 25, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 6, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_21 (Conv3D)           (None, 6, 25, 25, 32)     27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 6, 25, 25, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 6, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_18 (MaxPooling (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_22 (Conv3D)           (None, 3, 12, 12, 64)     55360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 3, 12, 12, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_23 (Conv3D)           (None, 3, 12, 12, 64)     110656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 3, 12, 12, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_19 (MaxPooling (None, 1, 6, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 843,157\n",
      "Trainable params: 842,693\n",
      "Non-trainable params: 464\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#write your model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8,kernel_size=(3,3,3), padding='same',input_shape=(24,100,100,3)))   #### 30 in case of full , image size 120,120\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(16, kernel_size=(3,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv3D(16, kernel_size=(3,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size=(3,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size=(3,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(3,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(3,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = optimizers.Adam()                                   #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20 \n",
    "img_cnt_idx=24\n",
    "train_generator = generator(train_path, train_doc, batch_size,img_cnt_idx)\n",
    "val_generator = generator(val_path, val_doc, batch_size,img_cnt_idx)\n",
    "\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)   # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 20\n",
      "Source path =  ./Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 3s - loss: 2.0454 - categorical_accuracy: 0.2797"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 66s 2s/step - loss: 2.0169 - categorical_accuracy: 0.2809 - val_loss: 3.2635 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-09-1212_14_36.139518/model-00001-2.01686-0.28088-3.26352-0.25000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 57s 2s/step - loss: 1.4183 - categorical_accuracy: 0.3544 - val_loss: 1.4160 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-09-1212_14_36.139518/model-00002-1.41831-0.35441-1.41597-0.36000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.3240 - categorical_accuracy: 0.3956 - val_loss: 1.1466 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-09-1212_14_36.139518/model-00003-1.32400-0.39559-1.14656-0.43000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 1.2760 - categorical_accuracy: 0.4118 - val_loss: 1.2450 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-09-1212_14_36.139518/model-00004-1.27599-0.41176-1.24499-0.43000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.2674 - categorical_accuracy: 0.4103 - val_loss: 1.4744 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-09-1212_14_36.139518/model-00005-1.26739-0.41029-1.47437-0.34000.h5\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 57s 2s/step - loss: 1.2065 - categorical_accuracy: 0.4324 - val_loss: 1.0266 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-09-1212_14_36.139518/model-00006-1.20651-0.43235-1.02655-0.58000.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.1316 - categorical_accuracy: 0.4559 - val_loss: 1.0965 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-09-1212_14_36.139518/model-00007-1.13163-0.45588-1.09650-0.52000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.1278 - categorical_accuracy: 0.4647 - val_loss: 1.9926 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-09-1212_14_36.139518/model-00008-1.12780-0.46471-1.99261-0.29000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.1401 - categorical_accuracy: 0.4809 - val_loss: 1.8885 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-09-1212_14_36.139518/model-00009-1.14006-0.48088-1.88849-0.31000.h5\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.0931 - categorical_accuracy: 0.4824 - val_loss: 1.8802 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-09-1212_14_36.139518/model-00010-1.09313-0.48235-1.88017-0.29000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.9914 - categorical_accuracy: 0.5426 - val_loss: 1.1905 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-09-1212_14_36.139518/model-00011-0.99135-0.54265-1.19045-0.39000.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.9173 - categorical_accuracy: 0.5676 - val_loss: 1.2801 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-09-1212_14_36.139518/model-00012-0.91728-0.56765-1.28008-0.39000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 57s 2s/step - loss: 0.9037 - categorical_accuracy: 0.5838 - val_loss: 1.1111 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-09-1212_14_36.139518/model-00013-0.90368-0.58382-1.11111-0.52000.h5\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 57s 2s/step - loss: 0.8858 - categorical_accuracy: 0.5809 - val_loss: 0.8317 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-09-1212_14_36.139518/model-00014-0.88584-0.58088-0.83171-0.66000.h5\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.8660 - categorical_accuracy: 0.6191 - val_loss: 0.8168 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-09-1212_14_36.139518/model-00015-0.86598-0.61912-0.81680-0.70000.h5\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.8156 - categorical_accuracy: 0.6338 - val_loss: 0.8671 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-09-1212_14_36.139518/model-00016-0.81563-0.63382-0.86711-0.68000.h5\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.8039 - categorical_accuracy: 0.6265 - val_loss: 0.8294 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-09-1212_14_36.139518/model-00017-0.80391-0.62647-0.82935-0.68000.h5\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.7315 - categorical_accuracy: 0.6971 - val_loss: 0.8002 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-09-1212_14_36.139518/model-00018-0.73151-0.69706-0.80018-0.61000.h5\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.7575 - categorical_accuracy: 0.6676 - val_loss: 1.0800 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-09-1212_14_36.139518/model-00019-0.75747-0.66765-1.08004-0.55000.h5\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.6677 - categorical_accuracy: 0.7324 - val_loss: 2.1888 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-09-1212_14_36.139518/model-00020-0.66765-0.73235-2.18883-0.34000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c1edb06d8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>with batch size 20 and epoch = 10 and 16 images the accuracy was for train : 56% and val : 16% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>with batch size 20 and epoch = 10 and 24 images the accuracy was for train : 58% and val : 16% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>with batch size 20 and epoch = 20 and 24 images(100,100) the accuracy was for train : 73% and val : 34%  even the models generated in between were not very impressive</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='background:yellow'>Model 4:  Trying another variation of model </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_24 (Conv3D)           (None, 24, 100, 100, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 24, 100, 100, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 24, 100, 100, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_20 (MaxPooling (None, 12, 50, 50, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_25 (Conv3D)           (None, 12, 50, 50, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 12, 50, 50, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 12, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_26 (Conv3D)           (None, 12, 50, 50, 32)    13856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 12, 50, 50, 32)    128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 12, 50, 50, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_21 (MaxPooling (None, 6, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_27 (Conv3D)           (None, 6, 25, 25, 64)     55360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 6, 25, 25, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 6, 25, 25, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_28 (Conv3D)           (None, 6, 25, 25, 64)     110656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 6, 25, 25, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 6, 25, 25, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_22 (MaxPooling (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 27648)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               7078144   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 7,264,165\n",
      "Trainable params: 7,263,797\n",
      "Non-trainable params: 368\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#write your model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8,kernel_size=(3,3,3), padding='same',input_shape=(24,100,100,3)))   #### 30 in case of full , image size 120,120\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(16, kernel_size=(3,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size=(3,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(3,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(3,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = optimizers.Adam()                                   #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20  \n",
    "img_cnt_idx=24\n",
    "train_generator = generator(train_path, train_doc, batch_size,img_cnt_idx)\n",
    "val_generator = generator(val_path, val_doc, batch_size,img_cnt_idx)\n",
    "\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)   # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path = Source path =  ./Project_data/train ; batch size = 20\n",
      " ./Project_data/val ; batch size = 20\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 3s - loss: 12.7278 - categorical_accuracy: 0.1938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 65s 2s/step - loss: 12.5717 - categorical_accuracy: 0.2044 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-09-1212_14_36.139518/model-00001-12.57168-0.20441-12.41093-0.23000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 55s 2s/step - loss: 12.8708 - categorical_accuracy: 0.2015 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-09-1212_14_36.139518/model-00002-12.87077-0.20147-12.41093-0.23000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 12.5626 - categorical_accuracy: 0.2206 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-09-1212_14_36.139518/model-00003-12.56263-0.22059-12.41093-0.23000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 12.8234 - categorical_accuracy: 0.2044 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-09-1212_14_36.139518/model-00004-12.82337-0.20441-12.41093-0.23000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 12.5863 - categorical_accuracy: 0.2191 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-09-1212_14_36.139518/model-00005-12.58634-0.21912-12.41093-0.23000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 12.6100 - categorical_accuracy: 0.2176 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-09-1212_14_36.139518/model-00006-12.61004-0.21765-12.41093-0.23000.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 12.9893 - categorical_accuracy: 0.1941 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-09-1212_14_36.139518/model-00007-12.98929-0.19412-12.41093-0.23000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 12.7523 - categorical_accuracy: 0.2088 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-09-1212_14_36.139518/model-00008-12.75226-0.20882-12.41093-0.23000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 12.8234 - categorical_accuracy: 0.2044 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-09-1212_14_36.139518/model-00009-12.82337-0.20441-12.41093-0.23000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 12.9182 - categorical_accuracy: 0.1985 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-09-1212_14_36.139518/model-00010-12.91818-0.19853-12.41093-0.23000.h5\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 12.7523 - categorical_accuracy: 0.2088 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-09-1212_14_36.139518/model-00011-12.75226-0.20882-12.41093-0.23000.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 12.7049 - categorical_accuracy: 0.2118 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-09-1212_14_36.139518/model-00012-12.70485-0.21176-12.41093-0.23000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 12.8708 - categorical_accuracy: 0.2015 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-09-1212_14_36.139518/model-00013-12.87077-0.20147-12.41093-0.23000.h5\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 12.5626 - categorical_accuracy: 0.2206 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-09-1212_14_36.139518/model-00014-12.56263-0.22059-12.41093-0.23000.h5\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 13.0604 - categorical_accuracy: 0.1897 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-09-1212_14_36.139518/model-00015-13.06040-0.18971-12.41093-0.23000.h5\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 57s 2s/step - loss: 12.7760 - categorical_accuracy: 0.2074 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-09-1212_14_36.139518/model-00016-12.77596-0.20735-12.41093-0.23000.h5\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 12.8945 - categorical_accuracy: 0.2000 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-09-1212_14_36.139518/model-00017-12.89448-0.20000-12.41093-0.23000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 12.7760 - categorical_accuracy: 0.2074 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-09-1212_14_36.139518/model-00018-12.77596-0.20735-12.41093-0.23000.h5\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 12.7523 - categorical_accuracy: 0.2088 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-09-1212_14_36.139518/model-00019-12.75226-0.20882-12.41093-0.23000.h5\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 12.8945 - categorical_accuracy: 0.2000 - val_loss: 12.4109 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-09-1212_14_36.139518/model-00020-12.89448-0.20000-12.41093-0.23000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c1cc419e8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>with batch size 16 and epoch = 10 and 24 images the accuracy was for train : 46% and val : 54% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>with batch size 20 and epoch = 20 and 24 images(100,100) the accuracy was for train : 20% and val : 23%</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='background:yellow'>Model 5:  Trying another variation of model </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_10 (Conv3D)           (None, 30, 100, 100, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 30, 100, 100, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 30, 100, 100, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3 (None, 15, 50, 50, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 15, 50, 50, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 15, 50, 50, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 15, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling (None, 7, 25, 25, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 7, 25, 25, 32)     13856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 7, 25, 25, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 7, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_13 (Conv3D)           (None, 3, 12, 12, 64)     55360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 3, 12, 12, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_14 (Conv3D)           (None, 3, 12, 12, 64)     110656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 3, 12, 12, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_12 (MaxPooling (None, 1, 6, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 808,357\n",
      "Trainable params: 807,989\n",
      "Non-trainable params: 368\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#write your model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8,kernel_size=(3,3,3), padding='same',input_shape=(30,100,100,3)))   #### 30 in case of full , image size 120,120\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(16, kernel_size=(3,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size=(3,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(3,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(3,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = optimizers.Adam()                                   #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50 \n",
    "img_cnt_idx=30\n",
    "train_generator = generator(train_path, train_doc, batch_size,img_cnt_idx)\n",
    "val_generator = generator(val_path, val_doc, batch_size,img_cnt_idx)\n",
    "\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)   # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val Source path =  ./Project_data/train ; batch size = 20\n",
      "; batch size = 20\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 4s - loss: 2.1835 - categorical_accuracy: 0.2703"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 80s 2s/step - loss: 2.1333 - categorical_accuracy: 0.2794 - val_loss: 1.7025 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-09-1306_06_01.454275/model-00001-2.13326-0.27941-1.70250-0.26000.h5\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 1.4066 - categorical_accuracy: 0.3647 - val_loss: 1.2353 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-09-1306_06_01.454275/model-00002-1.40664-0.36471-1.23529-0.47000.h5\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 73s 2s/step - loss: 1.3666 - categorical_accuracy: 0.3809 - val_loss: 1.1969 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-09-1306_06_01.454275/model-00003-1.36658-0.38088-1.19695-0.51000.h5\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 1.3183 - categorical_accuracy: 0.3662 - val_loss: 1.1720 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-09-1306_06_01.454275/model-00004-1.31829-0.36618-1.17196-0.39000.h5\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 72s 2s/step - loss: 1.2246 - categorical_accuracy: 0.4574 - val_loss: 1.1237 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-09-1306_06_01.454275/model-00005-1.22457-0.45735-1.12374-0.50000.h5\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 75s 2s/step - loss: 1.2406 - categorical_accuracy: 0.4147 - val_loss: 1.5919 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-09-1306_06_01.454275/model-00006-1.24062-0.41471-1.59187-0.29000.h5\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 75s 2s/step - loss: 1.1498 - categorical_accuracy: 0.4956 - val_loss: 3.1699 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-09-1306_06_01.454275/model-00007-1.14980-0.49559-3.16986-0.23000.h5\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 75s 2s/step - loss: 1.1118 - categorical_accuracy: 0.4735 - val_loss: 1.3993 - val_categorical_accuracy: 0.3300\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-09-1306_06_01.454275/model-00008-1.11184-0.47353-1.39930-0.33000.h5\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 76s 2s/step - loss: 1.0129 - categorical_accuracy: 0.5574 - val_loss: 0.9006 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-09-1306_06_01.454275/model-00009-1.01285-0.55735-0.90064-0.58000.h5\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 0.9927 - categorical_accuracy: 0.5691 - val_loss: 1.1506 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-09-1306_06_01.454275/model-00010-0.99273-0.56912-1.15056-0.50000.h5\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 79s 2s/step - loss: 0.9653 - categorical_accuracy: 0.5544 - val_loss: 1.5557 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-09-1306_06_01.454275/model-00011-0.96534-0.55441-1.55566-0.42000.h5\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 76s 2s/step - loss: 0.8297 - categorical_accuracy: 0.6324 - val_loss: 0.8023 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-09-1306_06_01.454275/model-00012-0.82974-0.63235-0.80227-0.70000.h5\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 77s 2s/step - loss: 0.8127 - categorical_accuracy: 0.6721 - val_loss: 1.0885 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-09-1306_06_01.454275/model-00013-0.81275-0.67206-1.08846-0.56000.h5\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 0.6733 - categorical_accuracy: 0.7471 - val_loss: 1.0961 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-09-1306_06_01.454275/model-00014-0.67331-0.74706-1.09608-0.61000.h5\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 77s 2s/step - loss: 0.7163 - categorical_accuracy: 0.7397 - val_loss: 1.1635 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-09-1306_06_01.454275/model-00015-0.71634-0.73971-1.16345-0.53000.h5\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 75s 2s/step - loss: 0.6694 - categorical_accuracy: 0.7500 - val_loss: 0.7501 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-09-1306_06_01.454275/model-00016-0.66942-0.75000-0.75010-0.68000.h5\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 0.6323 - categorical_accuracy: 0.7618 - val_loss: 1.2772 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-09-1306_06_01.454275/model-00017-0.63228-0.76176-1.27724-0.54000.h5\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 0.6062 - categorical_accuracy: 0.7824 - val_loss: 5.6396 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-09-1306_06_01.454275/model-00018-0.60620-0.78235-5.63964-0.24000.h5\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 75s 2s/step - loss: 0.5084 - categorical_accuracy: 0.8176 - val_loss: 1.0753 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-09-1306_06_01.454275/model-00019-0.50839-0.81765-1.07527-0.61000.h5\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 0.4673 - categorical_accuracy: 0.8221 - val_loss: 3.1366 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-09-1306_06_01.454275/model-00020-0.46733-0.82206-3.13661-0.41000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 0.3424 - categorical_accuracy: 0.8824 - val_loss: 0.8067 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00021: saving model to model_init_2020-09-1306_06_01.454275/model-00021-0.34239-0.88235-0.80672-0.62000.h5\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 0.2980 - categorical_accuracy: 0.9044 - val_loss: 0.6051 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00022: saving model to model_init_2020-09-1306_06_01.454275/model-00022-0.29803-0.90441-0.60511-0.78000.h5\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 75s 2s/step - loss: 0.2361 - categorical_accuracy: 0.9221 - val_loss: 0.6058 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00023: saving model to model_init_2020-09-1306_06_01.454275/model-00023-0.23612-0.92206-0.60581-0.76000.h5\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 73s 2s/step - loss: 0.2726 - categorical_accuracy: 0.9162 - val_loss: 0.5901 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00024: saving model to model_init_2020-09-1306_06_01.454275/model-00024-0.27263-0.91618-0.59013-0.77000.h5\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 0.1928 - categorical_accuracy: 0.9500 - val_loss: 0.5434 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00025: saving model to model_init_2020-09-1306_06_01.454275/model-00025-0.19282-0.95000-0.54337-0.80000.h5\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 0.1751 - categorical_accuracy: 0.9471 - val_loss: 0.5056 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00026: saving model to model_init_2020-09-1306_06_01.454275/model-00026-0.17506-0.94706-0.50557-0.81000.h5\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 77s 2s/step - loss: 0.1929 - categorical_accuracy: 0.9309 - val_loss: 0.5191 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00027: saving model to model_init_2020-09-1306_06_01.454275/model-00027-0.19286-0.93088-0.51913-0.81000.h5\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 0.1888 - categorical_accuracy: 0.9426 - val_loss: 0.9260 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00028: saving model to model_init_2020-09-1306_06_01.454275/model-00028-0.18885-0.94265-0.92604-0.67000.h5\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 76s 2s/step - loss: 0.1566 - categorical_accuracy: 0.9500 - val_loss: 0.5778 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00029: saving model to model_init_2020-09-1306_06_01.454275/model-00029-0.15656-0.95000-0.57784-0.79000.h5\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 75s 2s/step - loss: 0.1347 - categorical_accuracy: 0.9618 - val_loss: 0.6818 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00030: saving model to model_init_2020-09-1306_06_01.454275/model-00030-0.13467-0.96176-0.68180-0.76000.h5\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 0.1204 - categorical_accuracy: 0.9618 - val_loss: 0.5339 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00031: saving model to model_init_2020-09-1306_06_01.454275/model-00031-0.12036-0.96176-0.53392-0.81000.h5\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 0.1187 - categorical_accuracy: 0.9544 - val_loss: 0.5148 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00032: saving model to model_init_2020-09-1306_06_01.454275/model-00032-0.11865-0.95441-0.51479-0.81000.h5\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 0.1363 - categorical_accuracy: 0.9515 - val_loss: 0.5560 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00033: saving model to model_init_2020-09-1306_06_01.454275/model-00033-0.13630-0.95147-0.55597-0.81000.h5\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 0.1108 - categorical_accuracy: 0.9750 - val_loss: 0.5990 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00034: saving model to model_init_2020-09-1306_06_01.454275/model-00034-0.11082-0.97500-0.59903-0.79000.h5\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 0.1251 - categorical_accuracy: 0.9559 - val_loss: 0.5770 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00035: saving model to model_init_2020-09-1306_06_01.454275/model-00035-0.12510-0.95588-0.57702-0.80000.h5\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 75s 2s/step - loss: 0.1187 - categorical_accuracy: 0.9662 - val_loss: 0.5699 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00036: saving model to model_init_2020-09-1306_06_01.454275/model-00036-0.11871-0.96618-0.56987-0.80000.h5\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 75s 2s/step - loss: 0.1021 - categorical_accuracy: 0.9676 - val_loss: 0.5735 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00037: saving model to model_init_2020-09-1306_06_01.454275/model-00037-0.10213-0.96765-0.57352-0.80000.h5\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 75s 2s/step - loss: 0.0985 - categorical_accuracy: 0.9691 - val_loss: 0.5711 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00038: saving model to model_init_2020-09-1306_06_01.454275/model-00038-0.09846-0.96912-0.57111-0.81000.h5\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 75s 2s/step - loss: 0.1126 - categorical_accuracy: 0.9647 - val_loss: 0.5668 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00039: saving model to model_init_2020-09-1306_06_01.454275/model-00039-0.11255-0.96471-0.56675-0.81000.h5\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 75s 2s/step - loss: 0.1054 - categorical_accuracy: 0.9721 - val_loss: 0.5644 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00040: saving model to model_init_2020-09-1306_06_01.454275/model-00040-0.10540-0.97206-0.56445-0.82000.h5\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 75s 2s/step - loss: 0.1155 - categorical_accuracy: 0.9662 - val_loss: 0.5617 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00041: saving model to model_init_2020-09-1306_06_01.454275/model-00041-0.11545-0.96618-0.56167-0.82000.h5\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 75s 2s/step - loss: 0.1108 - categorical_accuracy: 0.9647 - val_loss: 0.5558 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00042: saving model to model_init_2020-09-1306_06_01.454275/model-00042-0.11082-0.96471-0.55580-0.82000.h5\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 75s 2s/step - loss: 0.1377 - categorical_accuracy: 0.9559 - val_loss: 0.5565 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00043: saving model to model_init_2020-09-1306_06_01.454275/model-00043-0.13773-0.95588-0.55646-0.82000.h5\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 73s 2s/step - loss: 0.1131 - categorical_accuracy: 0.9632 - val_loss: 0.5608 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00044: saving model to model_init_2020-09-1306_06_01.454275/model-00044-0.11314-0.96324-0.56079-0.82000.h5\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 76s 2s/step - loss: 0.1298 - categorical_accuracy: 0.9676 - val_loss: 0.5551 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00045: saving model to model_init_2020-09-1306_06_01.454275/model-00045-0.12982-0.96765-0.55512-0.82000.h5\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 75s 2s/step - loss: 0.1095 - categorical_accuracy: 0.9721 - val_loss: 0.5550 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00046: saving model to model_init_2020-09-1306_06_01.454275/model-00046-0.10953-0.97206-0.55502-0.82000.h5\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 75s 2s/step - loss: 0.0997 - categorical_accuracy: 0.9691 - val_loss: 0.5528 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00047: saving model to model_init_2020-09-1306_06_01.454275/model-00047-0.09969-0.96912-0.55280-0.83000.h5\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 0.1263 - categorical_accuracy: 0.9603 - val_loss: 0.5524 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00048: saving model to model_init_2020-09-1306_06_01.454275/model-00048-0.12626-0.96029-0.55240-0.83000.h5\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 74s 2s/step - loss: 0.1270 - categorical_accuracy: 0.9574 - val_loss: 0.5516 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00049: saving model to model_init_2020-09-1306_06_01.454275/model-00049-0.12704-0.95735-0.55160-0.83000.h5\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 73s 2s/step - loss: 0.1047 - categorical_accuracy: 0.9750 - val_loss: 0.5529 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00050: saving model to model_init_2020-09-1306_06_01.454275/model-00050-0.10466-0.97500-0.55290-0.83000.h5\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd4257e6748>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>with batch size 20 and epoch = 10 and 24 images the accuracy was for train : 78% and val : 51% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = blue>with batch size 20 and epoch = 10 and 24 images at (100,100)  the accuracy was for train : 85% and val : 82% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = blue>with batch size 20 and epoch = 30 and 24 images at (100,100)  the accuracy was for train : 95% and val : 82% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = blue>with batch size 20 and epoch = 30 and 16 images at (100,100)  the accuracy was for train : 69% and val : 73% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = blue>with batch size 20 and epoch = 30 and 30 images at (100,100)  the accuracy was for train : 98% and val : 82% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color = blue>with batch size 20 and epoch = 50 and 30 images at (100,100)  the accuracy was for train : 97.5% and val : 83%. So even with a lot of increase in epochs there was not much gain in the validation accuracy </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =red>this looks promising so will train it for more epochs</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='background:yellow'>Model 5a):  Trying another variation of model, kernels changed </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_20 (Conv3D)           (None, 24, 100, 100, 8)   200       \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 24, 100, 100, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 24, 100, 100, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_17 (MaxPooling (None, 12, 50, 50, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_21 (Conv3D)           (None, 12, 50, 50, 16)    1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 12, 50, 50, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 12, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_18 (MaxPooling (None, 6, 25, 25, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_22 (Conv3D)           (None, 6, 25, 25, 32)     4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 6, 25, 25, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 6, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_19 (MaxPooling (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_23 (Conv3D)           (None, 3, 12, 12, 64)     16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 3, 12, 12, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_24 (Conv3D)           (None, 3, 12, 12, 64)     4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 3, 12, 12, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_20 (MaxPooling (None, 1, 6, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 650,333\n",
      "Trainable params: 649,965\n",
      "Non-trainable params: 368\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#write your model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8,kernel_size=(2,2,2), padding='same',input_shape=(24,100,100,3)))   #### 30 in case of full , image size 120,120\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(16, kernel_size=(2,2,2),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size=(2,2,2),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(2,2,2),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(1,1,1),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = optimizers.Adam()                                   #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40 \n",
    "img_cnt_idx=24\n",
    "train_generator = generator(train_path, train_doc, batch_size,img_cnt_idx)\n",
    "val_generator = generator(val_path, val_doc, batch_size,img_cnt_idx)\n",
    "\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)   # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 20\n",
      "Source path =  ./Project_data/train ; batch size = 20\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 3s - loss: 0.3276 - categorical_accuracy: 0.8844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 63s 2s/step - loss: 0.3309 - categorical_accuracy: 0.8809 - val_loss: 0.5565 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-09-1306_06_01.454275/model-00001-0.33089-0.88088-0.55651-0.80000.h5\n",
      "Epoch 2/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.3476 - categorical_accuracy: 0.8632 - val_loss: 0.5999 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-09-1306_06_01.454275/model-00002-0.34756-0.86324-0.59992-0.80000.h5\n",
      "Epoch 3/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.3224 - categorical_accuracy: 0.8882 - val_loss: 0.6359 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-09-1306_06_01.454275/model-00003-0.32236-0.88824-0.63587-0.82000.h5\n",
      "Epoch 4/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.3070 - categorical_accuracy: 0.8809 - val_loss: 0.5459 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-09-1306_06_01.454275/model-00004-0.30697-0.88088-0.54591-0.81000.h5\n",
      "Epoch 5/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2714 - categorical_accuracy: 0.9074 - val_loss: 0.5931 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-09-1306_06_01.454275/model-00005-0.27137-0.90735-0.59313-0.78000.h5\n",
      "Epoch 6/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2665 - categorical_accuracy: 0.9015 - val_loss: 0.5946 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-09-1306_06_01.454275/model-00006-0.26650-0.90147-0.59461-0.79000.h5\n",
      "Epoch 7/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.2454 - categorical_accuracy: 0.9103 - val_loss: 0.5400 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-09-1306_06_01.454275/model-00007-0.24544-0.91029-0.53996-0.84000.h5\n",
      "Epoch 8/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2297 - categorical_accuracy: 0.9309 - val_loss: 0.5803 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-09-1306_06_01.454275/model-00009-0.22971-0.93088-0.58027-0.78000.h5\n",
      "Epoch 10/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2567 - categorical_accuracy: 0.9103 - val_loss: 0.5689 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-09-1306_06_01.454275/model-00010-0.25666-0.91029-0.56893-0.82000.h5\n",
      "Epoch 11/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2160 - categorical_accuracy: 0.9279 - val_loss: 0.5719 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-09-1306_06_01.454275/model-00011-0.21598-0.92794-0.57186-0.80000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 12/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2112 - categorical_accuracy: 0.9235 - val_loss: 0.5439 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-09-1306_06_01.454275/model-00012-0.21124-0.92353-0.54388-0.82000.h5\n",
      "Epoch 13/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1835 - categorical_accuracy: 0.9441 - val_loss: 0.5242 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-09-1306_06_01.454275/model-00013-0.18351-0.94412-0.52422-0.86000.h5\n",
      "Epoch 14/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2203 - categorical_accuracy: 0.9235 - val_loss: 0.5151 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-09-1306_06_01.454275/model-00014-0.22034-0.92353-0.51511-0.85000.h5\n",
      "Epoch 15/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2173 - categorical_accuracy: 0.9353 - val_loss: 0.5276 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-09-1306_06_01.454275/model-00015-0.21734-0.93529-0.52764-0.83000.h5\n",
      "Epoch 16/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2202 - categorical_accuracy: 0.9265 - val_loss: 0.5237 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-09-1306_06_01.454275/model-00016-0.22024-0.92647-0.52371-0.86000.h5\n",
      "Epoch 17/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.1882 - categorical_accuracy: 0.9324 - val_loss: 0.5390 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-09-1306_06_01.454275/model-00017-0.18823-0.93235-0.53896-0.82000.h5\n",
      "Epoch 18/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1757 - categorical_accuracy: 0.9500 - val_loss: 0.5412 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-09-1306_06_01.454275/model-00018-0.17573-0.95000-0.54124-0.83000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 19/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1769 - categorical_accuracy: 0.9382 - val_loss: 0.5326 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-09-1306_06_01.454275/model-00019-0.17690-0.93824-0.53258-0.83000.h5\n",
      "Epoch 20/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2191 - categorical_accuracy: 0.9147 - val_loss: 0.5270 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-09-1306_06_01.454275/model-00020-0.21909-0.91471-0.52697-0.83000.h5\n",
      "Epoch 21/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1645 - categorical_accuracy: 0.9529 - val_loss: 0.5202 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00021: saving model to model_init_2020-09-1306_06_01.454275/model-00021-0.16451-0.95294-0.52020-0.83000.h5\n",
      "Epoch 22/40\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.2163 - categorical_accuracy: 0.9221 - val_loss: 0.5174 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00022: saving model to model_init_2020-09-1306_06_01.454275/model-00022-0.21629-0.92206-0.51737-0.84000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 23/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1877 - categorical_accuracy: 0.9265 - val_loss: 0.5152 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00023: saving model to model_init_2020-09-1306_06_01.454275/model-00023-0.18772-0.92647-0.51521-0.85000.h5\n",
      "Epoch 24/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.1920 - categorical_accuracy: 0.9353 - val_loss: 0.5160 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00024: saving model to model_init_2020-09-1306_06_01.454275/model-00024-0.19203-0.93529-0.51604-0.85000.h5\n",
      "Epoch 25/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1751 - categorical_accuracy: 0.9382 - val_loss: 0.5156 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00025: saving model to model_init_2020-09-1306_06_01.454275/model-00025-0.17509-0.93824-0.51556-0.85000.h5\n",
      "Epoch 26/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1671 - categorical_accuracy: 0.9529 - val_loss: 0.5149 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00026: saving model to model_init_2020-09-1306_06_01.454275/model-00026-0.16711-0.95294-0.51486-0.84000.h5\n",
      "Epoch 27/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.1837 - categorical_accuracy: 0.9382 - val_loss: 0.5144 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00027: saving model to model_init_2020-09-1306_06_01.454275/model-00027-0.18369-0.93824-0.51436-0.85000.h5\n",
      "Epoch 28/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1804 - categorical_accuracy: 0.9279 - val_loss: 0.5137 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00028: saving model to model_init_2020-09-1306_06_01.454275/model-00028-0.18044-0.92794-0.51368-0.86000.h5\n",
      "Epoch 29/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1892 - categorical_accuracy: 0.9338 - val_loss: 0.5135 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00029: saving model to model_init_2020-09-1306_06_01.454275/model-00029-0.18924-0.93382-0.51347-0.86000.h5\n",
      "Epoch 30/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1657 - categorical_accuracy: 0.9279 - val_loss: 0.5135 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00030: saving model to model_init_2020-09-1306_06_01.454275/model-00030-0.16572-0.92794-0.51355-0.85000.h5\n",
      "Epoch 31/40\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.1617 - categorical_accuracy: 0.9515 - val_loss: 0.5125 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00031: saving model to model_init_2020-09-1306_06_01.454275/model-00031-0.16167-0.95147-0.51254-0.86000.h5\n",
      "Epoch 32/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1870 - categorical_accuracy: 0.9382 - val_loss: 0.5135 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00032: saving model to model_init_2020-09-1306_06_01.454275/model-00032-0.18705-0.93824-0.51353-0.85000.h5\n",
      "Epoch 33/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1630 - categorical_accuracy: 0.9471 - val_loss: 0.5122 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00033: saving model to model_init_2020-09-1306_06_01.454275/model-00033-0.16301-0.94706-0.51217-0.86000.h5\n",
      "Epoch 34/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1579 - categorical_accuracy: 0.9485 - val_loss: 0.5123 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00034: saving model to model_init_2020-09-1306_06_01.454275/model-00034-0.15789-0.94853-0.51230-0.86000.h5\n",
      "Epoch 35/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1888 - categorical_accuracy: 0.9441 - val_loss: 0.5135 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00035: saving model to model_init_2020-09-1306_06_01.454275/model-00035-0.18883-0.94412-0.51348-0.85000.h5\n",
      "Epoch 36/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1768 - categorical_accuracy: 0.9471 - val_loss: 0.5130 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00036: saving model to model_init_2020-09-1306_06_01.454275/model-00036-0.17677-0.94706-0.51295-0.85000.h5\n",
      "Epoch 37/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1943 - categorical_accuracy: 0.9324 - val_loss: 0.5130 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00037: saving model to model_init_2020-09-1306_06_01.454275/model-00037-0.19425-0.93235-0.51298-0.85000.h5\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 38/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1893 - categorical_accuracy: 0.9397 - val_loss: 0.5137 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00038: saving model to model_init_2020-09-1306_06_01.454275/model-00038-0.18927-0.93971-0.51365-0.85000.h5\n",
      "Epoch 39/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1863 - categorical_accuracy: 0.9324 - val_loss: 0.5129 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00039: saving model to model_init_2020-09-1306_06_01.454275/model-00039-0.18634-0.93235-0.51289-0.85000.h5\n",
      "Epoch 40/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1902 - categorical_accuracy: 0.9368 - val_loss: 0.5133 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00040: saving model to model_init_2020-09-1306_06_01.454275/model-00040-0.19021-0.93676-0.51330-0.85000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd444fcf518>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>with batch size 20 and epoch = 20 and 24 images the accuracy was for train : 88% and val : 81% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>with batch size 20 and epoch = 40 and 24 images the accuracy was for train : 93.6% and val : 85% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='background:yellow'>Model 5b):  Trying another variation of model further changes in the kernel </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_25 (Conv3D)           (None, 24, 100, 100, 16)  64        \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 24, 100, 100, 16)  64        \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 24, 100, 100, 16)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_21 (MaxPooling (None, 12, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_26 (Conv3D)           (None, 12, 50, 50, 32)    4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 12, 50, 50, 32)    128       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 12, 50, 50, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_22 (MaxPooling (None, 6, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_27 (Conv3D)           (None, 6, 25, 25, 64)     55360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 6, 25, 25, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 6, 25, 25, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_23 (MaxPooling (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_28 (Conv3D)           (None, 3, 12, 12, 128)    8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 3, 12, 12, 128)    512       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 3, 12, 12, 128)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_29 (Conv3D)           (None, 3, 12, 12, 128)    16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 3, 12, 12, 128)    512       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 3, 12, 12, 128)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_24 (MaxPooling (None, 1, 6, 6, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               1179904   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,299,301\n",
      "Trainable params: 1,298,565\n",
      "Non-trainable params: 736\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#write your model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(16,kernel_size=(1,1,1), padding='same',input_shape=(24,100,100,3)))   #### 30 in case of full , image size 120,120\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size=(2,2,2),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(3,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(128, kernel_size=(1,1,1),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv3D(128, kernel_size=(1,1,1),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = optimizers.Adam()                                   #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40 \n",
    "img_cnt_idx=24\n",
    "train_generator = generator(train_path, train_doc, batch_size,img_cnt_idx)\n",
    "val_generator = generator(val_path, val_doc, batch_size,img_cnt_idx)\n",
    "\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)   # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 20\n",
      "Source path =  ./Project_data/train ; batch size = 20\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 3s - loss: 3.5153 - categorical_accuracy: 0.2609"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 66s 2s/step - loss: 3.4107 - categorical_accuracy: 0.2632 - val_loss: 2.0034 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-09-1306_06_01.454275/model-00001-3.41070-0.26324-2.00337-0.23000.h5\n",
      "Epoch 2/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.5540 - categorical_accuracy: 0.3250 - val_loss: 1.4127 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-09-1306_06_01.454275/model-00002-1.55396-0.32500-1.41267-0.39000.h5\n",
      "Epoch 3/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 1.3768 - categorical_accuracy: 0.3691 - val_loss: 1.4704 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-09-1306_06_01.454275/model-00003-1.37684-0.36912-1.47039-0.37000.h5\n",
      "Epoch 4/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 1.2511 - categorical_accuracy: 0.4750 - val_loss: 1.1173 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-09-1306_06_01.454275/model-00004-1.25111-0.47500-1.11726-0.59000.h5\n",
      "Epoch 5/40\n",
      "34/34 [==============================] - 62s 2s/step - loss: 1.2201 - categorical_accuracy: 0.4735 - val_loss: 1.0016 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-09-1306_06_01.454275/model-00005-1.22011-0.47353-1.00156-0.61000.h5\n",
      "Epoch 6/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 1.0895 - categorical_accuracy: 0.5235 - val_loss: 0.8697 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-09-1306_06_01.454275/model-00006-1.08947-0.52353-0.86967-0.71000.h5\n",
      "Epoch 7/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 1.0809 - categorical_accuracy: 0.5632 - val_loss: 1.2349 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-09-1306_06_01.454275/model-00007-1.08086-0.56324-1.23485-0.59000.h5\n",
      "Epoch 8/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.9754 - categorical_accuracy: 0.6132 - val_loss: 1.2714 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-09-1306_06_01.454275/model-00008-0.97544-0.61324-1.27138-0.52000.h5\n",
      "Epoch 9/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.8972 - categorical_accuracy: 0.6485 - val_loss: 0.9826 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-09-1306_06_01.454275/model-00009-0.89715-0.64853-0.98263-0.59000.h5\n",
      "Epoch 10/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.8369 - categorical_accuracy: 0.6632 - val_loss: 0.7246 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-09-1306_06_01.454275/model-00010-0.83691-0.66324-0.72463-0.73000.h5\n",
      "Epoch 11/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.8028 - categorical_accuracy: 0.6765 - val_loss: 1.0323 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-09-1306_06_01.454275/model-00011-0.80276-0.67647-1.03235-0.63000.h5\n",
      "Epoch 12/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.7190 - categorical_accuracy: 0.7221 - val_loss: 0.9314 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-09-1306_06_01.454275/model-00012-0.71905-0.72206-0.93142-0.66000.h5\n",
      "Epoch 13/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.6860 - categorical_accuracy: 0.7500 - val_loss: 0.7006 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-09-1306_06_01.454275/model-00013-0.68604-0.75000-0.70064-0.74000.h5\n",
      "Epoch 14/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.5705 - categorical_accuracy: 0.8015 - val_loss: 0.7977 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-09-1306_06_01.454275/model-00014-0.57052-0.80147-0.79774-0.69000.h5\n",
      "Epoch 15/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.5181 - categorical_accuracy: 0.8074 - val_loss: 0.7888 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-09-1306_06_01.454275/model-00015-0.51807-0.80735-0.78885-0.73000.h5\n",
      "Epoch 16/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.5318 - categorical_accuracy: 0.8000 - val_loss: 0.9134 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-09-1306_06_01.454275/model-00016-0.53181-0.80000-0.91344-0.64000.h5\n",
      "Epoch 17/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.5031 - categorical_accuracy: 0.8221 - val_loss: 0.7340 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-09-1306_06_01.454275/model-00017-0.50311-0.82206-0.73404-0.78000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 18/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.4159 - categorical_accuracy: 0.8515 - val_loss: 0.7233 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-09-1306_06_01.454275/model-00018-0.41585-0.85147-0.72331-0.77000.h5\n",
      "Epoch 19/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.3521 - categorical_accuracy: 0.8721 - val_loss: 0.6402 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-09-1306_06_01.454275/model-00019-0.35212-0.87206-0.64020-0.80000.h5\n",
      "Epoch 20/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.3496 - categorical_accuracy: 0.8647 - val_loss: 0.5889 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-09-1306_06_01.454275/model-00020-0.34961-0.86471-0.58891-0.84000.h5\n",
      "Epoch 21/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.3373 - categorical_accuracy: 0.8750 - val_loss: 0.5875 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00021: saving model to model_init_2020-09-1306_06_01.454275/model-00021-0.33731-0.87500-0.58752-0.82000.h5\n",
      "Epoch 22/40\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.2515 - categorical_accuracy: 0.9118 - val_loss: 0.5838 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00022: saving model to model_init_2020-09-1306_06_01.454275/model-00022-0.25154-0.91176-0.58381-0.85000.h5\n",
      "Epoch 23/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.3010 - categorical_accuracy: 0.8853 - val_loss: 0.6788 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00023: saving model to model_init_2020-09-1306_06_01.454275/model-00023-0.30102-0.88529-0.67884-0.79000.h5\n",
      "Epoch 24/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2614 - categorical_accuracy: 0.8941 - val_loss: 0.5453 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00024: saving model to model_init_2020-09-1306_06_01.454275/model-00024-0.26137-0.89412-0.54527-0.84000.h5\n",
      "Epoch 25/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2430 - categorical_accuracy: 0.9074 - val_loss: 0.5360 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00025: saving model to model_init_2020-09-1306_06_01.454275/model-00025-0.24297-0.90735-0.53596-0.85000.h5\n",
      "Epoch 26/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2589 - categorical_accuracy: 0.9029 - val_loss: 0.6289 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00026: saving model to model_init_2020-09-1306_06_01.454275/model-00026-0.25885-0.90294-0.62892-0.84000.h5\n",
      "Epoch 27/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2061 - categorical_accuracy: 0.9338 - val_loss: 0.6017 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00027: saving model to model_init_2020-09-1306_06_01.454275/model-00027-0.20605-0.93382-0.60173-0.86000.h5\n",
      "Epoch 28/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2689 - categorical_accuracy: 0.9103 - val_loss: 0.5851 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00028: saving model to model_init_2020-09-1306_06_01.454275/model-00028-0.26894-0.91029-0.58512-0.84000.h5\n",
      "Epoch 29/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2457 - categorical_accuracy: 0.9147 - val_loss: 0.5901 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00029: saving model to model_init_2020-09-1306_06_01.454275/model-00029-0.24570-0.91471-0.59006-0.84000.h5\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 30/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2297 - categorical_accuracy: 0.9250 - val_loss: 0.5731 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00030: saving model to model_init_2020-09-1306_06_01.454275/model-00030-0.22970-0.92500-0.57312-0.85000.h5\n",
      "Epoch 31/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2561 - categorical_accuracy: 0.9044 - val_loss: 0.5762 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00031: saving model to model_init_2020-09-1306_06_01.454275/model-00031-0.25610-0.90441-0.57621-0.85000.h5\n",
      "Epoch 32/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2760 - categorical_accuracy: 0.8868 - val_loss: 0.5617 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00032: saving model to model_init_2020-09-1306_06_01.454275/model-00032-0.27604-0.88676-0.56168-0.86000.h5\n",
      "Epoch 33/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.2451 - categorical_accuracy: 0.9059 - val_loss: 0.5260 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00033: saving model to model_init_2020-09-1306_06_01.454275/model-00033-0.24510-0.90588-0.52602-0.86000.h5\n",
      "Epoch 34/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2543 - categorical_accuracy: 0.9029 - val_loss: 0.5380 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00034: saving model to model_init_2020-09-1306_06_01.454275/model-00034-0.25431-0.90294-0.53805-0.85000.h5\n",
      "Epoch 35/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2290 - categorical_accuracy: 0.9147 - val_loss: 0.5280 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00035: saving model to model_init_2020-09-1306_06_01.454275/model-00035-0.22899-0.91471-0.52801-0.83000.h5\n",
      "Epoch 36/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2371 - categorical_accuracy: 0.9103 - val_loss: 0.5192 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00036: saving model to model_init_2020-09-1306_06_01.454275/model-00036-0.23714-0.91029-0.51915-0.83000.h5\n",
      "Epoch 37/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1924 - categorical_accuracy: 0.9191 - val_loss: 0.5302 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00037: saving model to model_init_2020-09-1306_06_01.454275/model-00037-0.19236-0.91912-0.53024-0.84000.h5\n",
      "Epoch 38/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2292 - categorical_accuracy: 0.9103 - val_loss: 0.5260 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00038: saving model to model_init_2020-09-1306_06_01.454275/model-00038-0.22916-0.91029-0.52604-0.85000.h5\n",
      "Epoch 39/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2096 - categorical_accuracy: 0.9206 - val_loss: 0.5529 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00039: saving model to model_init_2020-09-1306_06_01.454275/model-00039-0.20960-0.92059-0.55287-0.85000.h5\n",
      "Epoch 40/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2128 - categorical_accuracy: 0.9294 - val_loss: 0.5524 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00040: saving model to model_init_2020-09-1306_06_01.454275/model-00040-0.21275-0.92941-0.55242-0.84000.h5\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd4449a4550>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>with batch size 20 and epoch = 40 and 24 images the accuracy was for train : 92.06% and val : 85% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='background:yellow'>Model 5c):  Trying another variation of model , changed the dropout value </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_10 (Conv3D)           (None, 24, 100, 100, 16)  64        \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 24, 100, 100, 16)  64        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 24, 100, 100, 16)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3 (None, 12, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 12, 50, 50, 32)    4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 12, 50, 50, 32)    128       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 12, 50, 50, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling (None, 6, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 6, 25, 25, 64)     55360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 6, 25, 25, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 6, 25, 25, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_13 (Conv3D)           (None, 3, 12, 12, 128)    8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 3, 12, 12, 128)    512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 3, 12, 12, 128)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_14 (Conv3D)           (None, 3, 12, 12, 128)    16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 3, 12, 12, 128)    512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 3, 12, 12, 128)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_12 (MaxPooling (None, 1, 6, 6, 128)      0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               1179904   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,299,301\n",
      "Trainable params: 1,298,565\n",
      "Non-trainable params: 736\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#write your model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(16,kernel_size=(1,1,1), padding='same',input_shape=(24,100,100,3)))   #### 30 in case of full , image size 120,120\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size=(2,2,2),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(3,3,3),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(128, kernel_size=(1,1,1),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv3D(128, kernel_size=(1,1,1),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = optimizers.Adam()                                   #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40 \n",
    "img_cnt_idx=24\n",
    "train_generator = generator(train_path, train_doc, batch_size,img_cnt_idx)\n",
    "val_generator = generator(val_path, val_doc, batch_size,img_cnt_idx)\n",
    "\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)   # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 20\n",
      "Source path =  ./Project_data/train ; batch size = 20\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 9s - loss: 2.1246 - categorical_accuracy: 0.3016 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 161s 5s/step - loss: 2.0799 - categorical_accuracy: 0.3118 - val_loss: 2.6232 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-09-1311_47_18.067053/model-00001-2.07990-0.31176-2.62316-0.25000.h5\n",
      "Epoch 2/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 1.2780 - categorical_accuracy: 0.4691 - val_loss: 1.1705 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-09-1311_47_18.067053/model-00002-1.27799-0.46912-1.17047-0.51000.h5\n",
      "Epoch 3/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.0359 - categorical_accuracy: 0.5824 - val_loss: 1.1558 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-09-1311_47_18.067053/model-00003-1.03594-0.58235-1.15582-0.47000.h5\n",
      "Epoch 4/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.8519 - categorical_accuracy: 0.6676 - val_loss: 0.8102 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-09-1311_47_18.067053/model-00004-0.85186-0.66765-0.81024-0.66000.h5\n",
      "Epoch 5/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.7786 - categorical_accuracy: 0.6882 - val_loss: 1.1441 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-09-1311_47_18.067053/model-00005-0.77864-0.68824-1.14406-0.56000.h5\n",
      "Epoch 6/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.5932 - categorical_accuracy: 0.7676 - val_loss: 0.9874 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-09-1311_47_18.067053/model-00006-0.59320-0.76765-0.98741-0.61000.h5\n",
      "Epoch 7/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.4823 - categorical_accuracy: 0.8353 - val_loss: 0.5928 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-09-1311_47_18.067053/model-00007-0.48232-0.83529-0.59281-0.72000.h5\n",
      "Epoch 8/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.3696 - categorical_accuracy: 0.8676 - val_loss: 0.8819 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-09-1311_47_18.067053/model-00008-0.36955-0.86765-0.88189-0.68000.h5\n",
      "Epoch 9/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.3921 - categorical_accuracy: 0.8544 - val_loss: 1.1343 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-09-1311_47_18.067053/model-00009-0.39210-0.85441-1.13431-0.64000.h5\n",
      "Epoch 10/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.2084 - categorical_accuracy: 0.9221 - val_loss: 0.6795 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-09-1311_47_18.067053/model-00010-0.20845-0.92206-0.67953-0.76000.h5\n",
      "Epoch 11/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.3619 - categorical_accuracy: 0.8676 - val_loss: 1.1161 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-09-1311_47_18.067053/model-00011-0.36186-0.86765-1.11607-0.63000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 12/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.1658 - categorical_accuracy: 0.9471 - val_loss: 0.5920 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-09-1311_47_18.067053/model-00012-0.16582-0.94706-0.59196-0.77000.h5\n",
      "Epoch 13/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.0908 - categorical_accuracy: 0.9706 - val_loss: 0.5135 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-09-1311_47_18.067053/model-00013-0.09076-0.97059-0.51354-0.79000.h5\n",
      "Epoch 14/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.0741 - categorical_accuracy: 0.9809 - val_loss: 0.4927 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-09-1311_47_18.067053/model-00014-0.07409-0.98088-0.49268-0.82000.h5\n",
      "Epoch 15/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.0552 - categorical_accuracy: 0.9897 - val_loss: 0.5221 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-09-1311_47_18.067053/model-00015-0.05520-0.98971-0.52213-0.81000.h5\n",
      "Epoch 16/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.0652 - categorical_accuracy: 0.9809 - val_loss: 0.4900 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-09-1311_47_18.067053/model-00016-0.06522-0.98088-0.48998-0.82000.h5\n",
      "Epoch 17/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.0549 - categorical_accuracy: 0.9882 - val_loss: 0.5609 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-09-1311_47_18.067053/model-00017-0.05493-0.98824-0.56088-0.80000.h5\n",
      "Epoch 18/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.0410 - categorical_accuracy: 0.9853 - val_loss: 0.5445 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-09-1311_47_18.067053/model-00018-0.04101-0.98529-0.54448-0.80000.h5\n",
      "Epoch 19/40\n",
      "34/34 [==============================] - 57s 2s/step - loss: 0.0376 - categorical_accuracy: 0.9941 - val_loss: 0.5192 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-09-1311_47_18.067053/model-00019-0.03760-0.99412-0.51917-0.79000.h5\n",
      "Epoch 20/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.0426 - categorical_accuracy: 0.9912 - val_loss: 0.5415 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-09-1311_47_18.067053/model-00020-0.04262-0.99118-0.54147-0.80000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 21/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.0261 - categorical_accuracy: 0.9985 - val_loss: 0.5631 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00021: saving model to model_init_2020-09-1311_47_18.067053/model-00021-0.02610-0.99853-0.56311-0.81000.h5\n",
      "Epoch 22/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.0283 - categorical_accuracy: 0.9926 - val_loss: 0.5722 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00022: saving model to model_init_2020-09-1311_47_18.067053/model-00022-0.02827-0.99265-0.57220-0.79000.h5\n",
      "Epoch 23/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.0282 - categorical_accuracy: 0.9985 - val_loss: 0.5632 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00023: saving model to model_init_2020-09-1311_47_18.067053/model-00023-0.02823-0.99853-0.56321-0.80000.h5\n",
      "Epoch 24/40\n",
      "34/34 [==============================] - 57s 2s/step - loss: 0.0325 - categorical_accuracy: 0.9897 - val_loss: 0.5724 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00024: saving model to model_init_2020-09-1311_47_18.067053/model-00024-0.03249-0.98971-0.57238-0.79000.h5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 25/40\n",
      "34/34 [==============================] - 57s 2s/step - loss: 0.0333 - categorical_accuracy: 0.9897 - val_loss: 0.5666 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00025: saving model to model_init_2020-09-1311_47_18.067053/model-00025-0.03333-0.98971-0.56660-0.79000.h5\n",
      "Epoch 26/40\n",
      "34/34 [==============================] - 57s 2s/step - loss: 0.0169 - categorical_accuracy: 0.9985 - val_loss: 0.5624 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00026: saving model to model_init_2020-09-1311_47_18.067053/model-00026-0.01687-0.99853-0.56241-0.77000.h5\n",
      "Epoch 27/40\n",
      "34/34 [==============================] - 57s 2s/step - loss: 0.0206 - categorical_accuracy: 0.9971 - val_loss: 0.5621 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00027: saving model to model_init_2020-09-1311_47_18.067053/model-00027-0.02058-0.99706-0.56206-0.78000.h5\n",
      "Epoch 28/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.0243 - categorical_accuracy: 0.9971 - val_loss: 0.5657 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00028: saving model to model_init_2020-09-1311_47_18.067053/model-00028-0.02435-0.99706-0.56566-0.78000.h5\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 29/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.0213 - categorical_accuracy: 0.9971 - val_loss: 0.5684 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00029: saving model to model_init_2020-09-1311_47_18.067053/model-00029-0.02126-0.99706-0.56835-0.78000.h5\n",
      "Epoch 30/40\n",
      "34/34 [==============================] - 56s 2s/step - loss: 0.0240 - categorical_accuracy: 0.9971 - val_loss: 0.5670 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00030: saving model to model_init_2020-09-1311_47_18.067053/model-00030-0.02403-0.99706-0.56700-0.78000.h5\n",
      "Epoch 31/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.0304 - categorical_accuracy: 0.9926 - val_loss: 0.5662 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00031: saving model to model_init_2020-09-1311_47_18.067053/model-00031-0.03044-0.99265-0.56622-0.78000.h5\n",
      "Epoch 32/40\n",
      "34/34 [==============================] - 56s 2s/step - loss: 0.0226 - categorical_accuracy: 0.9985 - val_loss: 0.5678 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00032: saving model to model_init_2020-09-1311_47_18.067053/model-00032-0.02261-0.99853-0.56783-0.79000.h5\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 33/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.0265 - categorical_accuracy: 0.9912 - val_loss: 0.5702 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00033: saving model to model_init_2020-09-1311_47_18.067053/model-00033-0.02649-0.99118-0.57025-0.78000.h5\n",
      "Epoch 34/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.0302 - categorical_accuracy: 0.9941 - val_loss: 0.5709 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00034: saving model to model_init_2020-09-1311_47_18.067053/model-00034-0.03024-0.99412-0.57092-0.78000.h5\n",
      "Epoch 35/40\n",
      "34/34 [==============================] - 57s 2s/step - loss: 0.0320 - categorical_accuracy: 0.9926 - val_loss: 0.5707 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00035: saving model to model_init_2020-09-1311_47_18.067053/model-00035-0.03200-0.99265-0.57069-0.79000.h5\n",
      "Epoch 36/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.0259 - categorical_accuracy: 0.9956 - val_loss: 0.5709 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00036: saving model to model_init_2020-09-1311_47_18.067053/model-00036-0.02592-0.99559-0.57087-0.79000.h5\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 37/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.0273 - categorical_accuracy: 0.9956 - val_loss: 0.5706 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00037: saving model to model_init_2020-09-1311_47_18.067053/model-00037-0.02734-0.99559-0.57062-0.78000.h5\n",
      "Epoch 38/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.0226 - categorical_accuracy: 0.9956 - val_loss: 0.5709 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00038: saving model to model_init_2020-09-1311_47_18.067053/model-00038-0.02261-0.99559-0.57091-0.79000.h5\n",
      "Epoch 39/40\n",
      "34/34 [==============================] - 57s 2s/step - loss: 0.0245 - categorical_accuracy: 0.9956 - val_loss: 0.5684 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00039: saving model to model_init_2020-09-1311_47_18.067053/model-00039-0.02452-0.99559-0.56842-0.78000.h5\n",
      "Epoch 40/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.0301 - categorical_accuracy: 0.9926 - val_loss: 0.5697 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00040: saving model to model_init_2020-09-1311_47_18.067053/model-00040-0.03011-0.99265-0.56973-0.78000.h5\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f187ea3abe0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>with batch size 20 and epoch = 40 and 24 images the best accuracy was for train : 98.09% and val : 82% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='background:yellow'>Model 5d):  Trying another variation of model , changed the dropout value ,different kernel </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_15 (Conv3D)           (None, 24, 100, 100, 8)   200       \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 24, 100, 100, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 24, 100, 100, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_13 (MaxPooling (None, 12, 50, 50, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_16 (Conv3D)           (None, 12, 50, 50, 16)    1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 12, 50, 50, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 12, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_14 (MaxPooling (None, 6, 25, 25, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_17 (Conv3D)           (None, 6, 25, 25, 32)     4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 6, 25, 25, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 6, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_15 (MaxPooling (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_18 (Conv3D)           (None, 3, 12, 12, 64)     16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 3, 12, 12, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_19 (Conv3D)           (None, 3, 12, 12, 64)     4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 3, 12, 12, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_16 (MaxPooling (None, 1, 6, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 650,333\n",
      "Trainable params: 649,965\n",
      "Non-trainable params: 368\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#write your model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8,kernel_size=(2,2,2), padding='same',input_shape=(24,100,100,3)))   #### 30 in case of full , image size 120,120\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(16, kernel_size=(2,2,2),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size=(2,2,2),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(2,2,2),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(1,1,1),  padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = optimizers.Adam()                                   #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40 \n",
    "img_cnt_idx=24\n",
    "train_generator = generator(train_path, train_doc, batch_size,img_cnt_idx)\n",
    "val_generator = generator(val_path, val_doc, batch_size,img_cnt_idx)\n",
    "\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)   # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 20\n",
      "Source path =  ./Project_data/train ; batch size = 20\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 3s - loss: 2.1741 - categorical_accuracy: 0.2766"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 64s 2s/step - loss: 2.1469 - categorical_accuracy: 0.2750 - val_loss: 1.7896 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-09-1311_47_18.067053/model-00001-2.14694-0.27500-1.78960-0.29000.h5\n",
      "Epoch 2/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.3201 - categorical_accuracy: 0.4088 - val_loss: 1.2880 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-09-1311_47_18.067053/model-00002-1.32007-0.40882-1.28802-0.43000.h5\n",
      "Epoch 3/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.1809 - categorical_accuracy: 0.4779 - val_loss: 1.0853 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-09-1311_47_18.067053/model-00003-1.18092-0.47794-1.08525-0.55000.h5\n",
      "Epoch 4/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.0896 - categorical_accuracy: 0.5324 - val_loss: 1.2190 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-09-1311_47_18.067053/model-00004-1.08959-0.53235-1.21902-0.44000.h5\n",
      "Epoch 5/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.9595 - categorical_accuracy: 0.5956 - val_loss: 1.3419 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-09-1311_47_18.067053/model-00005-0.95946-0.59559-1.34192-0.45000.h5\n",
      "Epoch 6/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.8016 - categorical_accuracy: 0.6721 - val_loss: 0.8783 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-09-1311_47_18.067053/model-00006-0.80157-0.67206-0.87833-0.64000.h5\n",
      "Epoch 7/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.6839 - categorical_accuracy: 0.7265 - val_loss: 0.8416 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-09-1311_47_18.067053/model-00007-0.68394-0.72647-0.84164-0.71000.h5\n",
      "Epoch 8/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.6492 - categorical_accuracy: 0.7456 - val_loss: 0.8199 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-09-1311_47_18.067053/model-00008-0.64921-0.74559-0.81985-0.68000.h5\n",
      "Epoch 9/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.5778 - categorical_accuracy: 0.7779 - val_loss: 1.3325 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-09-1311_47_18.067053/model-00009-0.57781-0.77794-1.33245-0.55000.h5\n",
      "Epoch 10/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.4698 - categorical_accuracy: 0.8176 - val_loss: 0.7905 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-09-1311_47_18.067053/model-00010-0.46985-0.81765-0.79054-0.75000.h5\n",
      "Epoch 11/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.4303 - categorical_accuracy: 0.8441 - val_loss: 1.7045 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-09-1311_47_18.067053/model-00011-0.43030-0.84412-1.70452-0.56000.h5\n",
      "Epoch 12/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.3293 - categorical_accuracy: 0.8956 - val_loss: 0.7752 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-09-1311_47_18.067053/model-00012-0.32925-0.89559-0.77523-0.75000.h5\n",
      "Epoch 13/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2769 - categorical_accuracy: 0.9000 - val_loss: 1.2182 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-09-1311_47_18.067053/model-00013-0.27694-0.90000-1.21824-0.67000.h5\n",
      "Epoch 14/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.2635 - categorical_accuracy: 0.9103 - val_loss: 0.8401 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-09-1311_47_18.067053/model-00014-0.26353-0.91029-0.84011-0.75000.h5\n",
      "Epoch 15/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2253 - categorical_accuracy: 0.9191 - val_loss: 1.5469 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-09-1311_47_18.067053/model-00015-0.22529-0.91912-1.54685-0.63000.h5\n",
      "Epoch 16/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2298 - categorical_accuracy: 0.9103 - val_loss: 1.2659 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-09-1311_47_18.067053/model-00016-0.22980-0.91029-1.26587-0.71000.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 17/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1295 - categorical_accuracy: 0.9676 - val_loss: 0.7151 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-09-1311_47_18.067053/model-00017-0.12946-0.96765-0.71512-0.79000.h5\n",
      "Epoch 18/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1059 - categorical_accuracy: 0.9706 - val_loss: 0.7312 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-09-1311_47_18.067053/model-00018-0.10589-0.97059-0.73120-0.79000.h5\n",
      "Epoch 19/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.0701 - categorical_accuracy: 0.9838 - val_loss: 0.7437 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-09-1311_47_18.067053/model-00019-0.07014-0.98382-0.74373-0.80000.h5\n",
      "Epoch 20/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.0735 - categorical_accuracy: 0.9750 - val_loss: 0.6953 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-09-1311_47_18.067053/model-00020-0.07350-0.97500-0.69531-0.82000.h5\n",
      "Epoch 21/40\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.0705 - categorical_accuracy: 0.9765 - val_loss: 0.8253 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00021: saving model to model_init_2020-09-1311_47_18.067053/model-00021-0.07055-0.97647-0.82527-0.79000.h5\n",
      "Epoch 22/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.0621 - categorical_accuracy: 0.9853 - val_loss: 0.8365 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00022: saving model to model_init_2020-09-1311_47_18.067053/model-00022-0.06212-0.98529-0.83654-0.80000.h5\n",
      "Epoch 23/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.0481 - categorical_accuracy: 0.9897 - val_loss: 0.7685 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00023: saving model to model_init_2020-09-1311_47_18.067053/model-00023-0.04812-0.98971-0.76846-0.81000.h5\n",
      "Epoch 24/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.0671 - categorical_accuracy: 0.9721 - val_loss: 0.7608 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00024: saving model to model_init_2020-09-1311_47_18.067053/model-00024-0.06714-0.97206-0.76080-0.80000.h5\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 25/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.0395 - categorical_accuracy: 0.9912 - val_loss: 0.7661 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00025: saving model to model_init_2020-09-1311_47_18.067053/model-00025-0.03953-0.99118-0.76610-0.79000.h5\n",
      "Epoch 26/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.0509 - categorical_accuracy: 0.9868 - val_loss: 0.7632 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00026: saving model to model_init_2020-09-1311_47_18.067053/model-00026-0.05088-0.98676-0.76319-0.78000.h5\n",
      "Epoch 27/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.0392 - categorical_accuracy: 0.9912 - val_loss: 0.7666 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00027: saving model to model_init_2020-09-1311_47_18.067053/model-00027-0.03920-0.99118-0.76657-0.81000.h5\n",
      "Epoch 28/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.0436 - categorical_accuracy: 0.9868 - val_loss: 0.7562 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00028: saving model to model_init_2020-09-1311_47_18.067053/model-00028-0.04358-0.98676-0.75623-0.82000.h5\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 29/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.0318 - categorical_accuracy: 0.9941 - val_loss: 0.7550 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00029: saving model to model_init_2020-09-1311_47_18.067053/model-00029-0.03180-0.99412-0.75504-0.81000.h5\n",
      "Epoch 30/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.0381 - categorical_accuracy: 0.9868 - val_loss: 0.7500 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00030: saving model to model_init_2020-09-1311_47_18.067053/model-00030-0.03807-0.98676-0.75002-0.82000.h5\n",
      "Epoch 31/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.0420 - categorical_accuracy: 0.9868 - val_loss: 0.7501 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00031: saving model to model_init_2020-09-1311_47_18.067053/model-00031-0.04200-0.98676-0.75014-0.82000.h5\n",
      "Epoch 32/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.0457 - categorical_accuracy: 0.9882 - val_loss: 0.7523 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00032: saving model to model_init_2020-09-1311_47_18.067053/model-00032-0.04569-0.98824-0.75226-0.82000.h5\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 33/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.0386 - categorical_accuracy: 0.9926 - val_loss: 0.7546 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00033: saving model to model_init_2020-09-1311_47_18.067053/model-00033-0.03862-0.99265-0.75459-0.82000.h5\n",
      "Epoch 34/40\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.0388 - categorical_accuracy: 0.9912 - val_loss: 0.7544 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00034: saving model to model_init_2020-09-1311_47_18.067053/model-00034-0.03883-0.99118-0.75435-0.83000.h5\n",
      "Epoch 35/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.0474 - categorical_accuracy: 0.9838 - val_loss: 0.7526 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00035: saving model to model_init_2020-09-1311_47_18.067053/model-00035-0.04744-0.98382-0.75264-0.83000.h5\n",
      "Epoch 36/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.0391 - categorical_accuracy: 0.9897 - val_loss: 0.7539 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00036: saving model to model_init_2020-09-1311_47_18.067053/model-00036-0.03910-0.98971-0.75387-0.83000.h5\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 37/40\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.0450 - categorical_accuracy: 0.9868 - val_loss: 0.7549 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00037: saving model to model_init_2020-09-1311_47_18.067053/model-00037-0.04502-0.98676-0.75491-0.83000.h5\n",
      "Epoch 38/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.0379 - categorical_accuracy: 0.9941 - val_loss: 0.7550 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00038: saving model to model_init_2020-09-1311_47_18.067053/model-00038-0.03785-0.99412-0.75501-0.83000.h5\n",
      "Epoch 39/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.0276 - categorical_accuracy: 0.9956 - val_loss: 0.7554 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00039: saving model to model_init_2020-09-1311_47_18.067053/model-00039-0.02764-0.99559-0.75540-0.83000.h5\n",
      "Epoch 40/40\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.0358 - categorical_accuracy: 0.9926 - val_loss: 0.7544 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00040: saving model to model_init_2020-09-1311_47_18.067053/model-00040-0.03582-0.99265-0.75436-0.83000.h5\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f184bfdccc0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>with batch size 20 and epoch = 40 and 24 images the best accuracy was for train : 99.26% and val : 83% </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style ='background:yellow'>Model 6 : Using a LSTM model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(8,kernel_size=(3,3), padding='same', activation='relu'),input_shape=(24,100,100,3)))   #### 30 in case of full , image size 120,120\n",
    "        \n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.5))\n",
    "        \n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "        \n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_5 (TimeDist (None, 24, 100, 100, 8)   224       \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 24, 100, 100, 8)   32        \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 24, 50, 50, 8)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 24, 20000)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                2564224   \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 2,566,917\n",
      "Trainable params: 2,566,901\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam()                                   #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "num_epochs = 20  \n",
    "img_cnt_idx=24\n",
    "train_generator = generator(train_path, train_doc, batch_size,img_cnt_idx)\n",
    "val_generator = generator(val_path, val_doc, batch_size,img_cnt_idx)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)   # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path = Source path =  ./Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n",
      " ./Project_data/val ; batch size = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 3s - loss: 1.7055 - categorical_accuracy: 0.2266"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 64s 2s/step - loss: 1.7066 - categorical_accuracy: 0.2250 - val_loss: 1.6075 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-09-1212_14_36.139518/model-00001-1.70660-0.22500-1.60750-0.27000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.5963 - categorical_accuracy: 0.2647 - val_loss: 1.6112 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-09-1212_14_36.139518/model-00002-1.59631-0.26471-1.61120-0.25000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.5307 - categorical_accuracy: 0.3191 - val_loss: 1.6309 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-09-1212_14_36.139518/model-00003-1.53067-0.31912-1.63088-0.21000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.5146 - categorical_accuracy: 0.3324 - val_loss: 1.4976 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-09-1212_14_36.139518/model-00004-1.51460-0.33235-1.49760-0.37000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.4606 - categorical_accuracy: 0.3647 - val_loss: 1.5546 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-09-1212_14_36.139518/model-00005-1.46056-0.36471-1.55459-0.27000.h5\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 1.4620 - categorical_accuracy: 0.3515 - val_loss: 1.5109 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-09-1212_14_36.139518/model-00006-1.46205-0.35147-1.51092-0.37000.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.3754 - categorical_accuracy: 0.4309 - val_loss: 1.4562 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-09-1212_14_36.139518/model-00007-1.37542-0.43088-1.45624-0.41000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.3165 - categorical_accuracy: 0.4603 - val_loss: 1.3432 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-09-1212_14_36.139518/model-00008-1.31654-0.46029-1.34319-0.45000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.2732 - categorical_accuracy: 0.4544 - val_loss: 1.3032 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-09-1212_14_36.139518/model-00009-1.27316-0.45441-1.30317-0.50000.h5\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.2374 - categorical_accuracy: 0.4985 - val_loss: 1.5274 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-09-1212_14_36.139518/model-00010-1.23741-0.49853-1.52735-0.35000.h5\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.2072 - categorical_accuracy: 0.5044 - val_loss: 1.1993 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-09-1212_14_36.139518/model-00011-1.20722-0.50441-1.19933-0.54000.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.1430 - categorical_accuracy: 0.5426 - val_loss: 1.2775 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-09-1212_14_36.139518/model-00012-1.14298-0.54265-1.27748-0.46000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 56s 2s/step - loss: 1.1307 - categorical_accuracy: 0.5426 - val_loss: 1.5933 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-09-1212_14_36.139518/model-00013-1.13075-0.54265-1.59326-0.36000.h5\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 57s 2s/step - loss: 1.1347 - categorical_accuracy: 0.5412 - val_loss: 1.1931 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-09-1212_14_36.139518/model-00014-1.13473-0.54118-1.19310-0.51000.h5\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 56s 2s/step - loss: 1.0620 - categorical_accuracy: 0.5882 - val_loss: 1.1348 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-09-1212_14_36.139518/model-00015-1.06198-0.58824-1.13478-0.53000.h5\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.0103 - categorical_accuracy: 0.6074 - val_loss: 2.0479 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-09-1212_14_36.139518/model-00016-1.01030-0.60735-2.04794-0.25000.h5\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.0983 - categorical_accuracy: 0.5691 - val_loss: 1.2550 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-09-1212_14_36.139518/model-00017-1.09834-0.56912-1.25503-0.48000.h5\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 57s 2s/step - loss: 1.0148 - categorical_accuracy: 0.6015 - val_loss: 1.6001 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-09-1212_14_36.139518/model-00018-1.01479-0.60147-1.60007-0.35000.h5\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 57s 2s/step - loss: 0.9924 - categorical_accuracy: 0.6162 - val_loss: 1.4191 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-09-1212_14_36.139518/model-00019-0.99236-0.61618-1.41906-0.44000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 57s 2s/step - loss: 0.9607 - categorical_accuracy: 0.6235 - val_loss: 1.3395 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-09-1212_14_36.139518/model-00020-0.96069-0.62353-1.33948-0.46000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c1b55a828>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>With batch size 20 and epoch = 10 and 16 images the accuracy was for train : 35% and val : 31% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>With batch size 20 and epoch = 10 and 24 images the accuracy was for train : 42% and val : 48% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>With batch size 16 and epoch = 20 and 24 images the accuracy was for train : 46% and val : 54% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>With batch size 16 and epoch = 20 and 24 images the accuracy was for train : 46% and val : 45%  and time taken : 56 mins</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>With batch size 20 and epoch = 20 and 24 images(100,100) the accuracy was for train : 62% and val : 46%  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='background:yellow'>Model 7: Adding another layer in the LSTM model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_9 (TimeDist (None, 24, 100, 100, 8)   224       \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 24, 100, 100, 8)   32        \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 24, 50, 50, 8)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 24, 50, 50, 16)    1168      \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 24, 50, 50, 16)    64        \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 24, 25, 25, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 24, 10000)         0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 32)                1284224   \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,288,149\n",
      "Trainable params: 1,288,101\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(8,kernel_size=(3,3), padding='same', activation='relu'),input_shape=(24,100,100,3)))   #### 30 in case of full , image size 120,120\n",
    "        \n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.5))\n",
    "        \n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "        \n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = optimizers.Adam()                                   #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20 \n",
    "img_cnt_idx=24\n",
    "train_generator = generator(train_path, train_doc, batch_size,img_cnt_idx)\n",
    "val_generator = generator(val_path, val_doc, batch_size,img_cnt_idx)\n",
    "\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)   # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 20\n",
      "Source path =  ./Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 3s - loss: 1.6441 - categorical_accuracy: 0.2406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 63s 2s/step - loss: 1.6332 - categorical_accuracy: 0.2485 - val_loss: 1.5366 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-09-1212_14_36.139518/model-00001-1.63320-0.24853-1.53662-0.34000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 56s 2s/step - loss: 1.5911 - categorical_accuracy: 0.2544 - val_loss: 1.5405 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-09-1212_14_36.139518/model-00002-1.59106-0.25441-1.54050-0.29000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 57s 2s/step - loss: 1.6161 - categorical_accuracy: 0.2456 - val_loss: 1.5920 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-09-1212_14_36.139518/model-00003-1.61614-0.24559-1.59203-0.23000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.5722 - categorical_accuracy: 0.2971 - val_loss: 1.5596 - val_categorical_accuracy: 0.2800\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-09-1212_14_36.139518/model-00004-1.57220-0.29706-1.55965-0.28000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 56s 2s/step - loss: 1.5345 - categorical_accuracy: 0.3309 - val_loss: 1.5288 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-09-1212_14_36.139518/model-00005-1.53454-0.33088-1.52884-0.27000.h5\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 56s 2s/step - loss: 1.4879 - categorical_accuracy: 0.3559 - val_loss: 1.4819 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-09-1212_14_36.139518/model-00006-1.48785-0.35588-1.48190-0.32000.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 55s 2s/step - loss: 1.4378 - categorical_accuracy: 0.3838 - val_loss: 1.4110 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-09-1212_14_36.139518/model-00007-1.43784-0.38382-1.41095-0.48000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 56s 2s/step - loss: 1.4686 - categorical_accuracy: 0.3441 - val_loss: 1.3544 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-09-1212_14_36.139518/model-00008-1.46863-0.34412-1.35443-0.52000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 1.3533 - categorical_accuracy: 0.4500 - val_loss: 1.3151 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-09-1212_14_36.139518/model-00009-1.35333-0.45000-1.31512-0.53000.h5\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.2949 - categorical_accuracy: 0.4471 - val_loss: 1.2399 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-09-1212_14_36.139518/model-00010-1.29494-0.44706-1.23991-0.58000.h5\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.3102 - categorical_accuracy: 0.4206 - val_loss: 1.3270 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-09-1212_14_36.139518/model-00011-1.31021-0.42059-1.32705-0.41000.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 57s 2s/step - loss: 1.2787 - categorical_accuracy: 0.4353 - val_loss: 1.1975 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-09-1212_14_36.139518/model-00012-1.27866-0.43529-1.19752-0.54000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.2096 - categorical_accuracy: 0.4912 - val_loss: 1.1808 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-09-1212_14_36.139518/model-00013-1.20957-0.49118-1.18085-0.55000.h5\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 57s 2s/step - loss: 1.2442 - categorical_accuracy: 0.4368 - val_loss: 1.1312 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-09-1212_14_36.139518/model-00014-1.24418-0.43676-1.13120-0.59000.h5\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 57s 2s/step - loss: 1.2099 - categorical_accuracy: 0.4721 - val_loss: 1.0907 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-09-1212_14_36.139518/model-00015-1.20994-0.47206-1.09068-0.58000.h5\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 57s 2s/step - loss: 1.2320 - categorical_accuracy: 0.4691 - val_loss: 1.0928 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-09-1212_14_36.139518/model-00016-1.23199-0.46912-1.09278-0.58000.h5\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 56s 2s/step - loss: 1.1654 - categorical_accuracy: 0.5044 - val_loss: 1.1368 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-09-1212_14_36.139518/model-00017-1.16542-0.50441-1.13677-0.49000.h5\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 56s 2s/step - loss: 1.1987 - categorical_accuracy: 0.4882 - val_loss: 1.7818 - val_categorical_accuracy: 0.3000\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-09-1212_14_36.139518/model-00018-1.19873-0.48824-1.78183-0.30000.h5\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.1429 - categorical_accuracy: 0.5074 - val_loss: 1.1899 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-09-1212_14_36.139518/model-00019-1.14288-0.50735-1.18990-0.47000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.0971 - categorical_accuracy: 0.5147 - val_loss: 1.0845 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-09-1212_14_36.139518/model-00020-1.09708-0.51471-1.08451-0.55000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c1b55af98>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>With batch size 20 and epoch = 10 and 16 images the accuracy was for train : 48% and val : 29% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>With batch size 20 and epoch = 20 and 24 images(100,100) the accuracy was for train : 51% and val : 55% </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='background:yellow'>Model 8: Using GRU</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(8,kernel_size=(3,3), padding='same', activation='relu'),input_shape=(24,100,100,3)))   #### 30 in case of full , image size 120,120\n",
    "        \n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(GRU(32))\n",
    "model.add(Dropout(0.5))\n",
    "        \n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "        \n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_16 (TimeDis (None, 24, 100, 100, 8)   224       \n",
      "_________________________________________________________________\n",
      "time_distributed_17 (TimeDis (None, 24, 100, 100, 8)   32        \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 24, 50, 50, 8)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 24, 20000)         0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                1923168   \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,925,861\n",
      "Trainable params: 1,925,845\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam()                                   #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 20\n",
    "num_epochs = 20  \n",
    "img_cnt_idx=24\n",
    "train_generator = generator(train_path, train_doc, batch_size,img_cnt_idx)\n",
    "val_generator = generator(val_path, val_doc, batch_size,img_cnt_idx)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)   # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 20\n",
      "Source path =  ./Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 3s - loss: 1.7995 - categorical_accuracy: 0.2531"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 64s 2s/step - loss: 1.8036 - categorical_accuracy: 0.2485 - val_loss: 1.6195 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-09-1212_14_36.139518/model-00001-1.80357-0.24853-1.61954-0.22000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.8001 - categorical_accuracy: 0.2456 - val_loss: 1.5951 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-09-1212_14_36.139518/model-00002-1.80014-0.24559-1.59511-0.29000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.7293 - categorical_accuracy: 0.2618 - val_loss: 1.5621 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-09-1212_14_36.139518/model-00003-1.72931-0.26176-1.56208-0.27000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.6812 - categorical_accuracy: 0.2765 - val_loss: 1.5815 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-09-1212_14_36.139518/model-00004-1.68118-0.27647-1.58146-0.31000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 1.6466 - categorical_accuracy: 0.2559 - val_loss: 1.5762 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-09-1212_14_36.139518/model-00005-1.64656-0.25588-1.57622-0.29000.h5\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.5551 - categorical_accuracy: 0.3191 - val_loss: 1.5354 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-09-1212_14_36.139518/model-00006-1.55515-0.31912-1.53538-0.32000.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.5367 - categorical_accuracy: 0.3412 - val_loss: 1.5640 - val_categorical_accuracy: 0.3300\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-09-1212_14_36.139518/model-00007-1.53669-0.34118-1.56399-0.33000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 1.5420 - categorical_accuracy: 0.2956 - val_loss: 1.8816 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-09-1212_14_36.139518/model-00008-1.54200-0.29559-1.88157-0.21000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.4429 - categorical_accuracy: 0.3750 - val_loss: 1.7839 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-09-1212_14_36.139518/model-00009-1.44287-0.37500-1.78387-0.23000.h5\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 1.4372 - categorical_accuracy: 0.3735 - val_loss: 1.9577 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-09-1212_14_36.139518/model-00010-1.43716-0.37353-1.95768-0.21000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.4167 - categorical_accuracy: 0.3632 - val_loss: 1.8528 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-09-1212_14_36.139518/model-00011-1.41669-0.36324-1.85275-0.24000.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.3882 - categorical_accuracy: 0.3941 - val_loss: 1.7756 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-09-1212_14_36.139518/model-00012-1.38817-0.39412-1.77560-0.24000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 1.3887 - categorical_accuracy: 0.4029 - val_loss: 1.6460 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-09-1212_14_36.139518/model-00013-1.38870-0.40294-1.64598-0.23000.h5\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.3378 - categorical_accuracy: 0.4074 - val_loss: 1.5831 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-09-1212_14_36.139518/model-00014-1.33782-0.40735-1.58313-0.27000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 1.2813 - categorical_accuracy: 0.4485 - val_loss: 1.4428 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-09-1212_14_36.139518/model-00015-1.28126-0.44853-1.44276-0.34000.h5\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 1.3100 - categorical_accuracy: 0.4338 - val_loss: 1.3460 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-09-1212_14_36.139518/model-00016-1.31003-0.43382-1.34595-0.48000.h5\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.2996 - categorical_accuracy: 0.4294 - val_loss: 1.3133 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-09-1212_14_36.139518/model-00017-1.29965-0.42941-1.31334-0.45000.h5\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 1.2774 - categorical_accuracy: 0.4485 - val_loss: 1.3026 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-09-1212_14_36.139518/model-00018-1.27742-0.44853-1.30257-0.47000.h5\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.2857 - categorical_accuracy: 0.4309 - val_loss: 1.3053 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-09-1212_14_36.139518/model-00019-1.28571-0.43088-1.30527-0.49000.h5\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 1.2678 - categorical_accuracy: 0.4294 - val_loss: 1.3020 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-09-1212_14_36.139518/model-00020-1.26785-0.42941-1.30198-0.49000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c16d299e8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blue>With batch size 20 and epoch = 10 and 16 images the accuracy was for train : 91% and val : 80% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blue>With batch size 20 and epoch = 10 and 24 images the accuracy was for train : 53% and val : 28% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blue>With batch size 20 and epoch = 20 and 16 images the accuracy was for train : 47% and val : 53% and time : 48 mins </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blue>With batch size 20 and epoch = 20 and 24 images(100,100) the accuracy was for train : 42% and val : 49%  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style ='background:yellow'> Model 9: Adding additional layer in the GRU </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_54 (TimeDis (None, 16, 120, 120, 8)   224       \n",
      "_________________________________________________________________\n",
      "time_distributed_55 (TimeDis (None, 16, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "time_distributed_56 (TimeDis (None, 16, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_57 (TimeDis (None, 16, 60, 60, 16)    1168      \n",
      "_________________________________________________________________\n",
      "time_distributed_58 (TimeDis (None, 16, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "time_distributed_59 (TimeDis (None, 16, 30, 30, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_60 (TimeDis (None, 16, 14400)         0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 32)                1385568   \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,389,493\n",
      "Trainable params: 1,389,445\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(8,kernel_size=(3,3), padding='same', activation='relu'),input_shape=(16,120,120,3)))   #### 30 in case of full , image size 120,120\n",
    "        \n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(GRU(32))\n",
    "model.add(Dropout(0.5))\n",
    "        \n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "        \n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = optimizers.Adam()                                   #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 20\n",
    "num_epochs = 20  \n",
    "img_cnt_idx=16\n",
    "train_generator = generator(train_path, train_doc, batch_size,img_cnt_idx)\n",
    "val_generator = generator(val_path, val_doc, batch_size,img_cnt_idx)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)   # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)   # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arunb\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\arunb\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n",
      "C:\\Users\\arunb\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:38: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "C:\\Users\\arunb\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:42: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 90s 3s/step - loss: 1.8566 - categorical_accuracy: 0.2456 - val_loss: 1.4437 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-09-0913_35_07.815118/model-00001-1.85664-0.24559-1.44369-0.32000.h5\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 83s 2s/step - loss: 1.6792 - categorical_accuracy: 0.2691 - val_loss: 1.6309 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-09-0913_35_07.815118/model-00002-1.67921-0.26912-1.63089-0.21000.h5\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 83s 2s/step - loss: 1.6097 - categorical_accuracy: 0.2912 - val_loss: 1.5224 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-09-0913_35_07.815118/model-00003-1.60968-0.29118-1.52241-0.26000.h5\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 88s 3s/step - loss: 1.5903 - categorical_accuracy: 0.2868 - val_loss: 1.7373 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-09-0913_35_07.815118/model-00004-1.59035-0.28676-1.73732-0.20000.h5\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 86s 3s/step - loss: 1.4711 - categorical_accuracy: 0.3603 - val_loss: 1.5449 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-09-0913_35_07.815118/model-00005-1.47108-0.36029-1.54489-0.31000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 84s 2s/step - loss: 1.4693 - categorical_accuracy: 0.3721 - val_loss: 1.6836 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-09-0913_35_07.815118/model-00006-1.46933-0.37206-1.68365-0.29000.h5\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 85s 3s/step - loss: 1.4255 - categorical_accuracy: 0.3868 - val_loss: 1.6674 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-09-0913_35_07.815118/model-00007-1.42551-0.38676-1.66736-0.43000.h5\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 82s 2s/step - loss: 1.3828 - categorical_accuracy: 0.3956 - val_loss: 1.4437 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-09-0913_35_07.815118/model-00008-1.38281-0.39559-1.44375-0.34000.h5\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 87s 3s/step - loss: 1.3484 - categorical_accuracy: 0.4368 - val_loss: 1.7896 - val_categorical_accuracy: 0.3500\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-09-0913_35_07.815118/model-00009-1.34844-0.43676-1.78964-0.35000.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 85s 2s/step - loss: 1.3362 - categorical_accuracy: 0.4515 - val_loss: 1.5323 - val_categorical_accuracy: 0.3800\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-09-0913_35_07.815118/model-00010-1.33617-0.45147-1.53228-0.38000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x139b59b7c48>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =blue>With batch size 20 and epoch = 10 and 16 images the accuracy was for train : 45% and val : 38% </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style ='background:yellow'>Model 10 : Using Transfer learning</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import vgg16 as vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 5s 0us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_20 (TimeDis (None, 24, 3, 3, 512)     14714688  \n",
      "_________________________________________________________________\n",
      "time_distributed_21 (TimeDis (None, 24, 3, 3, 512)     2048      \n",
      "_________________________________________________________________\n",
      "time_distributed_22 (TimeDis (None, 24, 1, 1, 512)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_23 (TimeDis (None, 24, 512)           0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 32)                52320     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 14,771,493\n",
      "Trainable params: 55,781\n",
      "Non-trainable params: 14,715,712\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "vgg_transfer = vgg.VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(vgg_transfer,input_shape=(24,100,100,3)))   #### 30 in case of full , image size 120,120     \n",
    "\n",
    "for layer in model.layers:\n",
    "     layer.trainable = False\n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "model.add(GRU(32))\n",
    "model.add(Dropout(0.5))\n",
    "        \n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "        \n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = optimizers.Adam()                                   #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 20\n",
    "num_epochs = 20  \n",
    "img_cnt_idx=24\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size,img_cnt_idx)\n",
    "val_generator = generator(val_path, val_doc, batch_size,img_cnt_idx)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)   # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 20\n",
      "Source path =  ./Project_data/train ; batch size = Epoch 1/20\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 4s - loss: 1.7982 - categorical_accuracy: 0.2359"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 74s 2s/step - loss: 1.7997 - categorical_accuracy: 0.2397 - val_loss: 1.5863 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-09-1212_14_36.139518/model-00001-1.79967-0.23971-1.58632-0.27000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 1.6195 - categorical_accuracy: 0.3235 - val_loss: 1.5261 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-09-1212_14_36.139518/model-00002-1.61952-0.32353-1.52611-0.26000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 1.5671 - categorical_accuracy: 0.3191 - val_loss: 1.4526 - val_categorical_accuracy: 0.3800\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-09-1212_14_36.139518/model-00003-1.56706-0.31912-1.45262-0.38000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 1.4662 - categorical_accuracy: 0.3662 - val_loss: 1.4117 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-09-1212_14_36.139518/model-00004-1.46616-0.36618-1.41167-0.41000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 1.3657 - categorical_accuracy: 0.4250 - val_loss: 1.3706 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-09-1212_14_36.139518/model-00005-1.36574-0.42500-1.37060-0.42000.h5\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 1.2237 - categorical_accuracy: 0.5059 - val_loss: 1.2912 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-09-1212_14_36.139518/model-00006-1.22372-0.50588-1.29121-0.47000.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 1.1855 - categorical_accuracy: 0.5044 - val_loss: 1.2800 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-09-1212_14_36.139518/model-00007-1.18545-0.50441-1.28002-0.45000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 1.1013 - categorical_accuracy: 0.5632 - val_loss: 1.2193 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-09-1212_14_36.139518/model-00008-1.10127-0.56324-1.21933-0.55000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.9965 - categorical_accuracy: 0.5838 - val_loss: 1.1927 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-09-1212_14_36.139518/model-00009-0.99655-0.58382-1.19275-0.54000.h5\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 63s 2s/step - loss: 0.8780 - categorical_accuracy: 0.6706 - val_loss: 1.1592 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-09-1212_14_36.139518/model-00010-0.87799-0.67059-1.15921-0.56000.h5\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 63s 2s/step - loss: 0.9332 - categorical_accuracy: 0.6485 - val_loss: 1.1356 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-09-1212_14_36.139518/model-00011-0.93325-0.64853-1.13556-0.58000.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.7963 - categorical_accuracy: 0.6956 - val_loss: 1.1186 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-09-1212_14_36.139518/model-00012-0.79630-0.69559-1.11855-0.56000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 63s 2s/step - loss: 0.6620 - categorical_accuracy: 0.7588 - val_loss: 1.1334 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-09-1212_14_36.139518/model-00013-0.66202-0.75882-1.13345-0.55000.h5\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 63s 2s/step - loss: 0.6119 - categorical_accuracy: 0.7824 - val_loss: 1.1149 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-09-1212_14_36.139518/model-00014-0.61194-0.78235-1.11490-0.59000.h5\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.6006 - categorical_accuracy: 0.7779 - val_loss: 1.2084 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-09-1212_14_36.139518/model-00015-0.60059-0.77794-1.20841-0.54000.h5\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 63s 2s/step - loss: 0.5443 - categorical_accuracy: 0.8074 - val_loss: 1.2099 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-09-1212_14_36.139518/model-00016-0.54433-0.80735-1.20993-0.54000.h5\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 63s 2s/step - loss: 0.5080 - categorical_accuracy: 0.8088 - val_loss: 1.2233 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-09-1212_14_36.139518/model-00017-0.50800-0.80882-1.22332-0.58000.h5\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.4208 - categorical_accuracy: 0.8588 - val_loss: 1.1903 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-09-1212_14_36.139518/model-00018-0.42077-0.85882-1.19028-0.59000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 63s 2s/step - loss: 0.3896 - categorical_accuracy: 0.8676 - val_loss: 1.1949 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-09-1212_14_36.139518/model-00019-0.38964-0.86765-1.19491-0.60000.h5\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.3553 - categorical_accuracy: 0.8838 - val_loss: 1.2112 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-09-1212_14_36.139518/model-00020-0.35532-0.88382-1.21122-0.58000.h5\n",
      "time taken0:20:59.680946\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"time taken\" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <font color=blue>With batch size 20 and epoch = 10 and 16 images the accuracy was for train : 72% and val : 71% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blue>With batch size 20 and epoch = 20 and 24 images(100,100) the accuracy was for train : 88% and val : 58% </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style ='background:yellow'>Model 11 : Using Transfer learning (removed dense layer at end)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_5 (TimeDist (None, 16, 3, 3, 512)     14714688  \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 16, 3, 3, 512)     2048      \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 16, 1, 1, 512)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 16, 512)           0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 32)                52320     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 14,769,221\n",
      "Trainable params: 53,509\n",
      "Non-trainable params: 14,715,712\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "vgg_transfer = vgg.VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(vgg_transfer,input_shape=(16,100,100,3)))   #### 30 in case of full , image size 120,120     \n",
    "\n",
    "for layer in model.layers:\n",
    "     layer.trainable = False\n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "model.add(GRU(32))\n",
    "       \n",
    "model.add(Dropout(0.5))\n",
    "        \n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = optimizers.Adam()                                   #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 20\n",
    "num_epochs = 20  \n",
    "img_cnt_idx=16\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size,img_cnt_idx)\n",
    "val_generator = generator(val_path, val_doc, batch_size,img_cnt_idx)\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)   # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 20\n",
      "Source path =  ./Project_data/train Epoch 1/20; batch size = 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 2s - loss: 1.8556 - categorical_accuracy: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 49s 1s/step - loss: 1.8447 - categorical_accuracy: 0.2485 - val_loss: 1.4537 - val_categorical_accuracy: 0.3600\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-09-1216_31_30.733418/model-00001-1.84470-0.24853-1.45375-0.36000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 41s 1s/step - loss: 1.3863 - categorical_accuracy: 0.4412 - val_loss: 1.2970 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-09-1216_31_30.733418/model-00002-1.38629-0.44118-1.29704-0.47000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 42s 1s/step - loss: 1.1058 - categorical_accuracy: 0.5706 - val_loss: 1.2188 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-09-1216_31_30.733418/model-00003-1.10578-0.57059-1.21875-0.55000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 42s 1s/step - loss: 0.9808 - categorical_accuracy: 0.6265 - val_loss: 1.1843 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-09-1216_31_30.733418/model-00004-0.98079-0.62647-1.18427-0.57000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 42s 1s/step - loss: 0.7717 - categorical_accuracy: 0.7309 - val_loss: 1.1461 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-09-1216_31_30.733418/model-00005-0.77173-0.73088-1.14607-0.60000.h5\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 41s 1s/step - loss: 0.7266 - categorical_accuracy: 0.7515 - val_loss: 1.1092 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-09-1216_31_30.733418/model-00006-0.72656-0.75147-1.10921-0.57000.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 42s 1s/step - loss: 0.6003 - categorical_accuracy: 0.8088 - val_loss: 1.1055 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-09-1216_31_30.733418/model-00007-0.60028-0.80882-1.10552-0.58000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 40s 1s/step - loss: 0.5558 - categorical_accuracy: 0.8147 - val_loss: 1.0767 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-09-1216_31_30.733418/model-00008-0.55584-0.81471-1.07672-0.60000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 42s 1s/step - loss: 0.4909 - categorical_accuracy: 0.8500 - val_loss: 1.0582 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-09-1216_31_30.733418/model-00009-0.49094-0.85000-1.05824-0.59000.h5\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 41s 1s/step - loss: 0.4020 - categorical_accuracy: 0.8809 - val_loss: 1.0483 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-09-1216_31_30.733418/model-00010-0.40201-0.88088-1.04826-0.60000.h5\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 41s 1s/step - loss: 0.3545 - categorical_accuracy: 0.8941 - val_loss: 1.0401 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-09-1216_31_30.733418/model-00011-0.35452-0.89412-1.04006-0.61000.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 42s 1s/step - loss: 0.3005 - categorical_accuracy: 0.9235 - val_loss: 1.0745 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-09-1216_31_30.733418/model-00012-0.30053-0.92353-1.07446-0.60000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 41s 1s/step - loss: 0.2773 - categorical_accuracy: 0.9265 - val_loss: 1.0620 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-09-1216_31_30.733418/model-00013-0.27730-0.92647-1.06200-0.62000.h5\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 42s 1s/step - loss: 0.2327 - categorical_accuracy: 0.9426 - val_loss: 1.0652 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-09-1216_31_30.733418/model-00014-0.23272-0.94265-1.06524-0.61000.h5\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 41s 1s/step - loss: 0.2067 - categorical_accuracy: 0.9515 - val_loss: 1.0559 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-09-1216_31_30.733418/model-00015-0.20673-0.95147-1.05591-0.63000.h5\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 41s 1s/step - loss: 0.1713 - categorical_accuracy: 0.9647 - val_loss: 1.0534 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-09-1216_31_30.733418/model-00016-0.17135-0.96471-1.05341-0.62000.h5\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 41s 1s/step - loss: 0.1785 - categorical_accuracy: 0.9588 - val_loss: 1.0553 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-09-1216_31_30.733418/model-00017-0.17846-0.95882-1.05534-0.63000.h5\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 42s 1s/step - loss: 0.1634 - categorical_accuracy: 0.9691 - val_loss: 1.0680 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-09-1216_31_30.733418/model-00018-0.16338-0.96912-1.06795-0.62000.h5\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 40s 1s/step - loss: 0.1482 - categorical_accuracy: 0.9779 - val_loss: 1.0618 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-09-1216_31_30.733418/model-00019-0.14819-0.97794-1.06182-0.62000.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 40s 1s/step - loss: 0.1478 - categorical_accuracy: 0.9779 - val_loss: 1.0637 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-09-1216_31_30.733418/model-00020-0.14784-0.97794-1.06368-0.61000.h5\n",
      "time taken0:13:57.214197\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"time taken\" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <font color=blue>With batch size 20 and epoch = 20 and 24 images(100,100) the accuracy was for train : 96% and val : 52% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <font color=blue>With batch size 20 and epoch = 20 and 16 images(100,100) the accuracy was for train : 97.8% and val : 61% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style ='background:yellow'>Model 12 :Use transfer Learning Second model </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras_applications/resnet50.py:263: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 7s 0us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_24 (TimeDis (None, 24, 3, 3, 2048)    23587712  \n",
      "_________________________________________________________________\n",
      "time_distributed_25 (TimeDis (None, 24, 3, 3, 2048)    8192      \n",
      "_________________________________________________________________\n",
      "time_distributed_26 (TimeDis (None, 24, 1, 1, 2048)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_27 (TimeDis (None, 24, 2048)          0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 32)                199776    \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 23,795,845\n",
      "Trainable params: 204,037\n",
      "Non-trainable params: 23,591,808\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "resnet_transfer = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(resnet_transfer,input_shape=(24,100,100,3)))   #### 30 in case of full , image size 120,120     \n",
    "\n",
    "for layer in model.layers:\n",
    "     layer.trainable = False\n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "model.add(GRU(32))\n",
    "model.add(Dropout(0.3))\n",
    "        \n",
    "       \n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = optimizers.Adam()                                   #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 20\n",
    "num_epochs = 20  \n",
    "img_cnt_idx=24\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size,img_cnt_idx)\n",
    "val_generator = generator(val_path, val_doc, batch_size,img_cnt_idx)\n",
    "\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)   # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ./Project_data/val ; batch size = 20\n",
      "Source path =  ./Project_data/train ; batch size = 20\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 3s - loss: 1.5672 - categorical_accuracy: 0.3547"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 73s 2s/step - loss: 1.5225 - categorical_accuracy: 0.3779 - val_loss: 1.9993 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-09-1212_14_36.139518/model-00001-1.52253-0.37794-1.99935-0.16000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 56s 2s/step - loss: 0.8373 - categorical_accuracy: 0.7074 - val_loss: 2.1090 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-09-1212_14_36.139518/model-00002-0.83730-0.70735-2.10898-0.16000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.5695 - categorical_accuracy: 0.8250 - val_loss: 1.8449 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-09-1212_14_36.139518/model-00003-0.56954-0.82500-1.84493-0.25000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.4225 - categorical_accuracy: 0.8809 - val_loss: 2.6462 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-09-1212_14_36.139518/model-00004-0.42250-0.88088-2.64620-0.16000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.3176 - categorical_accuracy: 0.9309 - val_loss: 2.4536 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-09-1212_14_36.139518/model-00005-0.31761-0.93088-2.45357-0.16000.h5\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2740 - categorical_accuracy: 0.9353 - val_loss: 2.5153 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-09-1212_14_36.139518/model-00006-0.27402-0.93529-2.51531-0.16000.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2442 - categorical_accuracy: 0.9426 - val_loss: 2.5992 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-09-1212_14_36.139518/model-00007-0.24425-0.94265-2.59917-0.16000.h5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.2058 - categorical_accuracy: 0.9574 - val_loss: 2.7524 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-09-1212_14_36.139518/model-00008-0.20577-0.95735-2.75244-0.16000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1798 - categorical_accuracy: 0.9676 - val_loss: 2.6206 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-09-1212_14_36.139518/model-00009-0.17981-0.96765-2.62064-0.16000.h5\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.1629 - categorical_accuracy: 0.9794 - val_loss: 2.7189 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-09-1212_14_36.139518/model-00010-0.16287-0.97941-2.71886-0.16000.h5\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.1458 - categorical_accuracy: 0.9868 - val_loss: 2.6218 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-09-1212_14_36.139518/model-00011-0.14581-0.98676-2.62178-0.16000.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 12/20\n",
      " 3/34 [=>............................] - ETA: 35s - loss: 0.1598 - categorical_accuracy: 0.9833"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <font color=blue>With batch size 20 and epoch = 10 and 16 images the accuracy was for train : 99% and val : 24% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <font color=blue>With batch size 20 and epoch = 10 and 16 images the accuracy was for train : 99% and val : 24% </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_20 (Conv3D)           (None, 24, 100, 100, 8)   200       \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 24, 100, 100, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 24, 100, 100, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_17 (MaxPooling (None, 12, 50, 50, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_21 (Conv3D)           (None, 12, 50, 50, 16)    1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 12, 50, 50, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 12, 50, 50, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_18 (MaxPooling (None, 6, 25, 25, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_22 (Conv3D)           (None, 6, 25, 25, 32)     4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 6, 25, 25, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 6, 25, 25, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_19 (MaxPooling (None, 3, 12, 12, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_23 (Conv3D)           (None, 3, 12, 12, 64)     16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 3, 12, 12, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_24 (Conv3D)           (None, 3, 12, 12, 64)     4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 3, 12, 12, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 3, 12, 12, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_20 (MaxPooling (None, 1, 6, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 650,333\n",
      "Trainable params: 649,965\n",
      "Non-trainable params: 368\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./model_init_2020-09-1306_06_01.454275/model-00040-0.19021-0.93676-0.51330-0.85000.h5')\n",
    "optimiser = optimizers.Adam()                                   #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 20\n",
    "num_epochs = 20  \n",
    "img_cnt_idx=24\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size,img_cnt_idx)\n",
    "val_generator = generator(val_path, val_doc, batch_size,img_cnt_idx)\n",
    "\n",
    "\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)   # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Source path =  ./Project_data/train ; batch size = 20\n",
      "./Project_data/val ; batch size = 20\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/34 [===========================>..] - ETA: 10s - loss: 0.5706 - categorical_accuracy: 0.8203"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 175s 5s/step - loss: 0.5605 - categorical_accuracy: 0.8221 - val_loss: 0.6284 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-09-1316_06_46.594852/model-00001-0.56052-0.82206-0.62843-0.77000.h5\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.3644 - categorical_accuracy: 0.8779 - val_loss: 0.7985 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-09-1316_06_46.594852/model-00002-0.36437-0.87794-0.79852-0.78000.h5\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.2407 - categorical_accuracy: 0.9206 - val_loss: 0.7865 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-09-1316_06_46.594852/model-00003-0.24070-0.92059-0.78654-0.76000.h5\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.2410 - categorical_accuracy: 0.9191 - val_loss: 0.8665 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-09-1316_06_46.594852/model-00004-0.24102-0.91912-0.86651-0.80000.h5\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.3325 - categorical_accuracy: 0.8868 - val_loss: 0.7453 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-09-1316_06_46.594852/model-00005-0.33253-0.88676-0.74532-0.80000.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.2718 - categorical_accuracy: 0.9162 - val_loss: 0.5750 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-09-1316_06_46.594852/model-00006-0.27184-0.91618-0.57499-0.84000.h5\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1791 - categorical_accuracy: 0.9485 - val_loss: 0.6292 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-09-1316_06_46.594852/model-00007-0.17905-0.94853-0.62918-0.82000.h5\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1537 - categorical_accuracy: 0.9426 - val_loss: 0.5856 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-09-1316_06_46.594852/model-00008-0.15373-0.94265-0.58562-0.86000.h5\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1279 - categorical_accuracy: 0.9559 - val_loss: 0.6300 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-09-1316_06_46.594852/model-00009-0.12788-0.95588-0.63002-0.83000.h5\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1522 - categorical_accuracy: 0.9529 - val_loss: 0.6414 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-09-1316_06_46.594852/model-00010-0.15217-0.95294-0.64139-0.81000.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1458 - categorical_accuracy: 0.9500 - val_loss: 0.6470 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-09-1316_06_46.594852/model-00011-0.14580-0.95000-0.64701-0.84000.h5\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1361 - categorical_accuracy: 0.9618 - val_loss: 0.6215 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-09-1316_06_46.594852/model-00012-0.13612-0.96176-0.62152-0.83000.h5\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1015 - categorical_accuracy: 0.9691 - val_loss: 0.6058 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-09-1316_06_46.594852/model-00013-0.10151-0.96912-0.60577-0.83000.h5\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 61s 2s/step - loss: 0.1287 - categorical_accuracy: 0.9632 - val_loss: 0.6034 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-09-1316_06_46.594852/model-00014-0.12867-0.96324-0.60344-0.85000.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 63s 2s/step - loss: 0.1233 - categorical_accuracy: 0.9574 - val_loss: 0.6008 - val_categorical_accuracy: 0.8700\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-09-1316_06_46.594852/model-00015-0.12328-0.95735-0.60078-0.87000.h5\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 62s 2s/step - loss: 0.1244 - categorical_accuracy: 0.9603 - val_loss: 0.5995 - val_categorical_accuracy: 0.8700\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-09-1316_06_46.594852/model-00016-0.12441-0.96029-0.59946-0.87000.h5\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 60s 2s/step - loss: 0.1306 - categorical_accuracy: 0.9544 - val_loss: 0.5989 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-09-1316_06_46.594852/model-00017-0.13060-0.95441-0.59888-0.86000.h5\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 59s 2s/step - loss: 0.1279 - categorical_accuracy: 0.9485 - val_loss: 0.5988 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-09-1316_06_46.594852/model-00018-0.12788-0.94853-0.59885-0.86000.h5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.1203 - categorical_accuracy: 0.9676 - val_loss: 0.5971 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-09-1316_06_46.594852/model-00019-0.12033-0.96765-0.59706-0.86000.h5\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 58s 2s/step - loss: 0.1267 - categorical_accuracy: 0.9588 - val_loss: 0.5962 - val_categorical_accuracy: 0.8600\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-09-1316_06_46.594852/model-00020-0.12670-0.95882-0.59624-0.86000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3f36bc5c18>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
